{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  CAMEL Memory Cookbook"
      ],
      "metadata": {
        "id": "w5_pa5kKPzAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also check this cookbook in colab [here](https://colab.research.google.com/drive/1ixGItEqQGkp09_TuV_8SGmz65WHBr5r5?usp=sharing)"
      ],
      "metadata": {
        "id": "zgqKzSB-PXzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "lSXa_rQQQzBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Memory module in CAMEL provides a flexible and powerful system for storing, retrieving, and managing information for AI agents. It enables agents to maintain context across conversations and retrieve relevant information from past interactions, enhancing the coherence and relevance of AI responses."
      ],
      "metadata": {
        "id": "OnmoAw2vQG8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started\n"
      ],
      "metadata": {
        "id": "l4MP9uUNQ2kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "kETDiaP2Rrdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure you have CAMEL AI installed in your Python environment:"
      ],
      "metadata": {
        "id": "Cg96MkbcRtQR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZVmDAK6MPefC"
      },
      "outputs": [],
      "source": [
        "pip install camel-ai[all]==0.1.6.8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Setting Up API Keys"
      ],
      "metadata": {
        "id": "MyTTCe3IR_Lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll need to set up your API keys for OpenAI."
      ],
      "metadata": {
        "id": "REqzgGL9SEaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for the API key securely\n",
        "openai_api_key = getpass('Enter your API key: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNBFEXc-R-0s",
        "outputId": "52d481de-9a81-4c00-e3db-9c468e6b5b22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your API key: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "LRojeqp7dP1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the Memory module in your agent:\n",
        "\n",
        "1. Choose an appropriate AgentMemory implementation (`ChatHistoryMemory`, `VectorDBMemory`, or `LongtermAgentMemory`).\n",
        "2. Initialize the memory with a context creator and any necessary parameters.\n",
        "3. Use `write_records()` to add new information to the memory.\n",
        "4. Use `retrieve()` to get relevant context for the agent's next action.\n",
        "5. Use `get_context()` to obtain the formatted context for the agent."
      ],
      "metadata": {
        "id": "_sJV9GFldTBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting `LongtermAgentMemory`:"
      ],
      "metadata": {
        "id": "aiypnpqJoZ4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.memories import (\n",
        "    ChatHistoryBlock,\n",
        "    LongtermAgentMemory,\n",
        "    MemoryRecord,\n",
        "    ScoreBasedContextCreator,\n",
        "    VectorDBBlock,\n",
        ")\n",
        "from camel.messages import BaseMessage\n",
        "from camel.types import ModelType, OpenAIBackendRole\n",
        "from camel.utils import OpenAITokenCounter\n",
        "\n",
        "# Initialize the memory\n",
        "memory = LongtermAgentMemory(\n",
        "    context_creator=ScoreBasedContextCreator(\n",
        "        token_counter=OpenAITokenCounter(ModelType.GPT_4O_MINI),\n",
        "        token_limit=1024,\n",
        "    ),\n",
        "    chat_history_block=ChatHistoryBlock(),\n",
        "    vector_db_block=VectorDBBlock(),\n",
        ")\n",
        "\n",
        "# Create and write new records\n",
        "records = [\n",
        "    MemoryRecord(\n",
        "        message=BaseMessage.make_user_message(\n",
        "            role_name=\"User\",\n",
        "            meta_dict=None,\n",
        "            content=\"What is CAMEL AI?\",\n",
        "        ),\n",
        "        role_at_backend=OpenAIBackendRole.USER,\n",
        "    ),\n",
        "    MemoryRecord(\n",
        "        message=BaseMessage.make_assistant_message(\n",
        "            role_name=\"Agent\",\n",
        "            meta_dict=None,\n",
        "            content=\"CAMEL-AI.org is the 1st LLM multi-agent framework and \"\n",
        "            \"an open-source community dedicated to finding the scaling law \"\n",
        "            \"of agents.\",\n",
        "        ),\n",
        "        role_at_backend=OpenAIBackendRole.ASSISTANT,\n",
        "    ),\n",
        "]\n",
        "memory.write_records(records)\n",
        "\n",
        "# Get context for the agent\n",
        "context, token_count = memory.get_context()\n",
        "\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAdDeN7DdUgr",
        "outputId": "0575acdb-a999-4818-f90c-de1e9b35890c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'What is CAMEL AI?'}, {'role': 'assistant', 'content': 'CAMEL-AI.org is the 1st LLM multi-agent framework and an open-source community dedicated to finding the scaling law of agents.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEiFYV-yez5j",
        "outputId": "2340282b-1024-49de-be67-2f9ed34ee18c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding `LongtermAgentMemory` to your `ChatAgent`:"
      ],
      "metadata": {
        "id": "K-P9rD6Foe5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.agents import ChatAgent\n",
        "\n",
        "# Define system message for the agent\n",
        "sys_msg = BaseMessage.make_assistant_message(\n",
        "    role_name='Agent',\n",
        "    content='You are a curious agent wondering about the universe.',\n",
        ")\n",
        "\n",
        "# Initialize agent\n",
        "agent = ChatAgent(system_message=sys_msg)\n",
        "\n",
        "# Set memory to the agent\n",
        "agent.memory = memory\n",
        "\n",
        "\n",
        "# Define a user message\n",
        "usr_msg = BaseMessage.make_user_message(\n",
        "    role_name='User',\n",
        "    content=\"Tell me which is the 1st LLM multi-agent framework based on what we have discussed\",\n",
        ")\n",
        "\n",
        "# Sending the message to the agent\n",
        "response = agent.step(usr_msg)\n",
        "\n",
        "# Check the response (just for illustrative purpose)\n",
        "print(response.msgs[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mp-KXFdGpKZt",
        "outputId": "e0e6c8b2-142e-4a3a-d34d-2feafebac137"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CAMEL AI is recognized as the first LLM (Large Language Model) multi-agent framework. It is an open-source community initiative focused on exploring the scaling laws of agents, enabling the development and interaction of multiple AI agents in a collaborative environment. This framework allows researchers and developers to experiment with various configurations and interactions among agents, facilitating advancements in AI capabilities and understanding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5cuumB_bo46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Components"
      ],
      "metadata": {
        "id": "biR7z_UuVIpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MemoryRecord"
      ],
      "metadata": {
        "id": "yhZN9zrGVMGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic data unit in the `CAMEL` memory system.\n",
        "\n",
        "Attributes:\n",
        "\n",
        "*   `message`: The main content of the record (`BaseMessage`)\n",
        "*   `role_at_backend`: The role this message played at the OpenAI backend (`OpenAIBackendRole`)\n",
        "*   `uuid`: A unique identifier for the record\n",
        "*   `extra_info`: Additional key-value pairs for extra information\n",
        "\n",
        "Methods:\n",
        "\n",
        "*   `from_dict()`: Static method to construct a `MemoryRecord` from a dictionary\n",
        "*   `to_dict()`: Convert the `MemoryRecord` to a dictionary for serialization\n",
        "*   `to_openai_message()`: Convert the record to an `OpenAIMessage` object\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VpEvPzJTVOpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ContextRecord"
      ],
      "metadata": {
        "id": "rsHhKQCZcHwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of memory retrieval from `AgentMemory`, which is used by `ContextCreator` to select records for creating the context for the agent.\n",
        "\n",
        "Attributes:\n",
        "*   `memory_record`: A `MemoryRecord`\n",
        "*   `score`: A float value representing the relevance or importance of the record\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JH7QcPbOcJZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MemoryBlock (Abstract Base Class)"
      ],
      "metadata": {
        "id": "flkwS-h7czVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serves as the fundamental component within the agent memory system. `MemoryBlock` class follows the \"Composite Design pattern\" so that you can compose objects into tree structures and then work with these structures as if they were individual objects.\n",
        "\n",
        "- `write_records()`: Write multiple records to memory\n",
        "- `write_record()`: Write a single record to memory\n",
        "- `clear()`: Remove all records from memory"
      ],
      "metadata": {
        "id": "o5hrNX61czyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BaseContextCreator (Abstract Base Class)"
      ],
      "metadata": {
        "id": "e2pFWs_fc3x9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the context creation strategies when retrieval messages exceed the model's context length.\n",
        "\n",
        "- `token_counter`: For counting tokens in a message\n",
        "- `token_limit`: The maximum number of tokens allowed in the generated context\n",
        "- `create_context()`: Create conversational context from chat history"
      ],
      "metadata": {
        "id": "RK1jULOHc5nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AgentMemory (Abstract Base Class)"
      ],
      "metadata": {
        "id": "MkS-HtLPc7Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A specialized form of MemoryBlock designed for direct integration with an agent.\n",
        "\n",
        "- `retrieve()`: Get a list of `ContextRecords` from memory\n",
        "- `get_context_creator()`: Get a context creator\n",
        "- `get_context()`: Get chat context with a proper size for the agent"
      ],
      "metadata": {
        "id": "-3qXes7bc8qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory Block Implementations"
      ],
      "metadata": {
        "id": "JfFD7p_bc-Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatHistoryBlock"
      ],
      "metadata": {
        "id": "_RxaH6mrc_4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintains a record of chat histories.\n",
        "\n",
        "Initialization:\n",
        "\n",
        "- `storage`: Optional `BaseKeyValueStorage` (default: `InMemoryKeyValueStorage`)\n",
        "- `keep_rate`: Float value for scoring historical messages (default: `0.9`)\n",
        "\n",
        "Methods:\n",
        "\n",
        "- `retrieve()`: Get recent chat history with optional window size\n",
        "- `write_records()`: Write new records to chat history\n",
        "- `clear()`: Remove all chat messages\n",
        "\n",
        "Use Case: Ideal for maintaining recent conversation context and flow."
      ],
      "metadata": {
        "id": "B1zbOVXjdBcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VectorDBBlock"
      ],
      "metadata": {
        "id": "uCtnDn9LdELC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses vector embeddings for maintaining and retrieving information.\n",
        "\n",
        "Initialization:\n",
        "\n",
        "- `storage`: Optional `BaseVectorStorage` (default: `QdrantStorage`)\n",
        "- `embedding`: Optional `BaseEmbedding` (default: `OpenAIEmbedding`)\n",
        "\n",
        "Methods:\n",
        "\n",
        "- `retrieve()`: Get similar records based on a keyword\n",
        "- `write_records()`: Convert and write new records to the vector database\n",
        "- `clear()`: Remove all records from the vector database\n",
        "\n",
        "Use Case: Better for retrieving semantically relevant information across a large set of messages.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "1. Storage Mechanism: `ChatHistoryBlock` uses key-value storage, `VectorDBBlock` uses vector database storage.\n",
        "2. Retrieval Method: `ChatHistoryBlock` retrieves based on recency, `VectorDBBlock` retrieves based on semantic similarity.\n",
        "3. Data Representation: `ChatHistoryBlock` stores messages in original form, `VectorDBBlock` converts to vector embeddings."
      ],
      "metadata": {
        "id": "OG5uFL-RdF53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Memory Implementations"
      ],
      "metadata": {
        "id": "1DS8v4wedIMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatHistoryMemory\n",
        "\n",
        "An AgentMemory implementation that wraps ChatHistoryBlock.\n",
        "\n",
        "Initialization:\n",
        "\n",
        "- `context_creator`: `BaseContextCreator`\n",
        "- `storage`: Optional `BaseKeyValueStorage`\n",
        "- `window_size`: Optional int for retrieval window\n",
        "\n",
        "Methods:\n",
        "\n",
        "- `retrieve()`: Get recent chat messages\n",
        "- `write_records()`: Write new records to chat history\n",
        "- `get_context_creator()`: Get the context creator\n",
        "- `clear()`: Remove all chat messages"
      ],
      "metadata": {
        "id": "9oKvM-_gdKRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VectorDBMemory\n",
        "\n",
        "An `AgentMemory` implementation that wraps `VectorDBBlock`.\n",
        "\n",
        "Initialization:\n",
        "\n",
        "- `context_creator`: BaseContextCreator\n",
        "- `storage`: Optional BaseVectorStorage\n",
        "- `retrieve_limit`: int for maximum number of retrieved messages (default: `3`)\n",
        "\n",
        "Methods:\n",
        "\n",
        "- `retrieve()`: Get relevant messages from the vector database\n",
        "- `write_records()`: Write new records and update current topic\n",
        "- `get_context_creator()`: Get the context creator"
      ],
      "metadata": {
        "id": "JOorsYrjdMDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LongtermAgentMemory\n",
        "\n",
        "Combines ChatHistoryMemory and VectorDBMemory for comprehensive memory management.\n",
        "\n",
        "Initialization:\n",
        "\n",
        "- `context_creator`: `BaseContextCreator`\n",
        "- `chat_history_block`: Optional `ChatHistoryBlock`\n",
        "- `vector_db_block`: Optional `VectorDBBlock`\n",
        "- `retrieve_limit`: int for maximum number of retrieved messages (default: `3`)\n",
        "\n",
        "Methods:\n",
        "\n",
        "- `retrieve()`: Get context from both chat history and vector database\n",
        "- `write_records()`: Write new records to both chat history and vector database\n",
        "- `get_context_creator()`: Get the context creator\n",
        "- `clear()`: Remove all records from both memory blocks"
      ],
      "metadata": {
        "id": "vlj-hzakdOEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Topics"
      ],
      "metadata": {
        "id": "w0ndTJske1mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing Context Creator"
      ],
      "metadata": {
        "id": "9q8EXq2ge6Dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create custom context creators by subclassing `BaseContextCreator`:"
      ],
      "metadata": {
        "id": "zUjztraTe7aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.memories import BaseContextCreator\n",
        "\n",
        "class MyCustomContextCreator(BaseContextCreator):\n",
        "    @property\n",
        "    def token_counter(self):\n",
        "        # Implement your token counting logic\n",
        "        return\n",
        "\n",
        "    @property\n",
        "    def token_limit(self):\n",
        "        return 1000  # Or any other limit\n",
        "\n",
        "    def create_context(self, records):\n",
        "        # Implement your context creation logic\n",
        "        pass"
      ],
      "metadata": {
        "id": "LX4fyovCe89R"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing Vector Database Block"
      ],
      "metadata": {
        "id": "N_HM5wyFfEtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For `VectorDBBlock`, you can customize it by adjusting the embedding models or vector storages:"
      ],
      "metadata": {
        "id": "CxcjHzArfFYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.embeddings import OpenAIEmbedding\n",
        "from camel.memories import VectorDBBlock\n",
        "from camel.storages import QdrantStorage\n",
        "\n",
        "vector_db = VectorDBBlock(\n",
        "    embedding=OpenAIEmbedding(),\n",
        "    storage=QdrantStorage(vector_dim=OpenAIEmbedding().get_output_dim()),\n",
        ")"
      ],
      "metadata": {
        "id": "iQlxwVM7fJEp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Considerations"
      ],
      "metadata": {
        "id": "u_pkVP5Eg2vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For large-scale applications, consider using persistent storage backends instead of in-memory storage.\n",
        "- Optimize your context creator to balance between context relevance and token limits.\n",
        "- When using `VectorDBMemory`, be mindful of the trade-off between retrieval accuracy and speed as the database grows."
      ],
      "metadata": {
        "id": "_4C93qAng3SP"
      }
    }
  ]
}
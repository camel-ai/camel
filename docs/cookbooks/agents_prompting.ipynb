{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  CAMEL Prompting Cookbook\n"
      ],
      "metadata": {
        "id": "SQMBlxsBv1cH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also check this cookbook in colab [here](https://colab.research.google.com/drive/1VcjPuy2UEYm0xLdriT7OMGt6I2yX9z32?usp=sharing)"
      ],
      "metadata": {
        "id": "XFyl4eJue3Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *TLDR*"
      ],
      "metadata": {
        "id": "-nCRZU5sInDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CAMEL Prompt module offers a powerful way to guide AI models in producing accurate, contextually relevant, and personalized outputs. This cookbook demonstrates how to use various prompt templates, create custom prompts, and leverage different prompt dictionaries for tasks ranging from role-playing to code generation, evaluation, and more. By mastering the Prompt module, you can significantly enhance your AI agents' capabilities and tailor them to specific tasks."
      ],
      "metadata": {
        "id": "AwnfLQvMvtXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation"
      ],
      "metadata": {
        "id": "XuZwbB1IIuUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure you have CAMEL AI installed in your Python environment:"
      ],
      "metadata": {
        "id": "gIJK4Lu5Iwm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install camel-ai==0.2.0"
      ],
      "metadata": {
        "id": "HTKnWg9Xv_y4",
        "collapsed": true,
        "outputId": "b16b819e-5578-484e-ba2e-14d3d022bd69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camel-ai==0.2.0\n",
            "  Downloading camel_ai-0.2.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting anthropic<0.30.0,>=0.29.0 (from camel-ai==0.2.0)\n",
            "  Downloading anthropic-0.29.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting colorama<1,>=0 (from camel-ai==0.2.0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting curl_cffi==0.6.2 (from camel-ai==0.2.0)\n",
            "  Downloading curl_cffi-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from camel-ai==0.2.0)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: eval-type-backport==0.2.0 in /usr/local/lib/python3.10/dist-packages (from camel-ai==0.2.0) (0.2.0)\n",
            "Collecting groq<0.6.0,>=0.5.0 (from camel-ai==0.2.0)\n",
            "  Downloading groq-0.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting ipykernel<7.0.0,>=6.0.0 (from camel-ai==0.2.0)\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jsonschema<5,>=4 in /usr/local/lib/python3.10/dist-packages (from camel-ai==0.2.0) (4.23.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from camel-ai==0.2.0) (1.26.4)\n",
            "Collecting openai<2.0.0,>=1.45.0 (from camel-ai==0.2.0)\n",
            "  Downloading openai-1.45.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pandoc (from camel-ai==0.2.0)\n",
            "  Downloading pandoc-2.4.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pathlib<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from camel-ai==0.2.0) (1.0.1)\n",
            "Collecting protobuf<5,>=4 (from camel-ai==0.2.0)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.10/dist-packages (from camel-ai==0.2.0) (2.9.1)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from camel-ai==0.2.0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from curl_cffi==0.6.2->camel-ai==0.2.0) (1.17.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from curl_cffi==0.6.2->camel-ai==0.2.0) (2024.8.30)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (4.12.2)\n",
            "Collecting comm>=0.1.1 (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (5.7.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (6.3.3)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4->camel-ai==0.2.0) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4->camel-ai==0.2.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4->camel-ai==0.2.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5,>=4->camel-ai==0.2.0) (0.20.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.45.0->camel-ai==0.2.0) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9->camel-ai==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9->camel-ai==0.2.0) (2.23.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->camel-ai==0.2.0) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<0.8.0,>=0.7.0->camel-ai==0.2.0) (2.32.3)\n",
            "Collecting plumbum (from pandoc->camel-ai==0.2.0)\n",
            "  Downloading plumbum-1.8.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ply (from pandoc->camel-ai==0.2.0)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12.0->curl_cffi==0.6.2->camel-ai==0.2.0) (2.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (4.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->camel-ai==0.2.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<0.8.0,>=0.7.0->camel-ai==0.2.0) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (0.24.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.30.0,>=0.29.0->camel-ai==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel<7.0.0,>=6.0.0->camel-ai==0.2.0) (1.16.0)\n",
            "Downloading camel_ai-0.2.0-py3-none-any.whl (366 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curl_cffi-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anthropic-0.29.2-py3-none-any.whl (863 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m863.5/863.5 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Downloading groq-0.5.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.45.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading plumbum-1.8.3-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.6/127.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pandoc\n",
            "  Building wheel for pandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandoc: filename=pandoc-2.4-py3-none-any.whl size=34792 sha256=0ec706522338a68492cdb70e39e7c7566dd6f3a1880b361d0b8359c7d5c819c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/79/8c/5d7a023cc8df1aa0381c1739d69da18ae7f90c08b2dc9a1bf5\n",
            "Successfully built pandoc\n",
            "Installing collected packages: ply, protobuf, plumbum, jiter, jedi, h11, docstring-parser, comm, colorama, tiktoken, pandoc, httpcore, curl_cffi, ipykernel, httpx, openai, groq, anthropic, camel-ai\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: docstring-parser\n",
            "    Found existing installation: docstring_parser 0.16\n",
            "    Uninstalling docstring_parser-0.16:\n",
            "      Successfully uninstalled docstring_parser-0.16\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anthropic-0.29.2 camel-ai-0.2.0 colorama-0.4.6 comm-0.2.2 curl_cffi-0.6.2 docstring-parser-0.15 groq-0.5.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 ipykernel-6.29.5 jedi-0.19.1 jiter-0.5.0 openai-1.45.0 pandoc-2.4 plumbum-1.8.3 ply-3.11 protobuf-4.25.4 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Setting Up API Keys"
      ],
      "metadata": {
        "id": "MyTTCe3IR_Lr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll need to set up your API keys for OpenAI."
      ],
      "metadata": {
        "id": "REqzgGL9SEaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for the API key securely\n",
        "openai_api_key = getpass('Enter your API key: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNBFEXc-R-0s",
        "outputId": "ba72676a-f143-4bfa-94d5-68fab0e059ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your API key: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started with Prompt Templates\n",
        "CAMEL offers a wide range of pre-defined prompt templates that you can use to quickly create specialized AI agents. Let's start with a basic example using the TaskSpecifyAgent with the AI_SOCIETY task type."
      ],
      "metadata": {
        "id": "GB5JK4Xs2fe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.agents import TaskSpecifyAgent\n",
        "from camel.configs import ChatGPTConfig\n",
        "from camel.models import ModelFactory\n",
        "from camel.types import ModelPlatformType, ModelType, TaskType\n",
        "\n",
        "# Set up the model\n",
        "model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.OPENAI,\n",
        "    model_type=ModelType.GPT_4O_MINI,\n",
        "    model_config_dict=ChatGPTConfig().as_dict(),\n",
        ")\n",
        "\n",
        "# Create a task specify agent\n",
        "task_specify_agent = TaskSpecifyAgent(\n",
        "    model=model, task_type=TaskType.AI_SOCIETY\n",
        ")\n",
        "\n",
        "# Run the agent with a task prompt\n",
        "specified_task_prompt = task_specify_agent.run(\n",
        "    task_prompt=\"Improving stage presence and performance skills\",\n",
        "    meta_dict=dict(\n",
        "        assistant_role=\"Musician\", user_role=\"Student\", word_limit=100\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(f\"Specified task prompt:\\n{specified_task_prompt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "754A8R7X2rnP",
        "outputId": "b9597bb8-0495-4b71-d50c-392b8b55fb9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Specified task prompt:\n",
            "Design a personalized workshop where the Musician guides the Student through dynamic exercises, including breath control, body language, and audience engagement techniques. Incorporate role-playing scenarios and video feedback sessions to refine their stage presence, culminating in a mini-performance to showcase newfound skills and boost confidence in front of a live audience.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Custom Prompts\n",
        "CAMEL also allows you to create your own custom prompts. Here's an example of how to create and use a custom prompt template:"
      ],
      "metadata": {
        "id": "JlX8L7-H3Csb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.agents import TaskSpecifyAgent\n",
        "from camel.configs import ChatGPTConfig\n",
        "from camel.models import ModelFactory\n",
        "from camel.prompts import TextPrompt\n",
        "from camel.types import ModelPlatformType, ModelType\n",
        "\n",
        "# Set up the model\n",
        "model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.OPENAI,\n",
        "    model_type=ModelType.GPT_4O_MINI,\n",
        "    model_config_dict=ChatGPTConfig().as_dict(),\n",
        ")\n",
        "\n",
        "# Create a custom prompt template\n",
        "my_prompt_template = TextPrompt(\n",
        "    'Here is a task: I\\'m a {occupation} and I want to {task}. Help me to make this task more specific.'\n",
        ")\n",
        "\n",
        "# Create a task specify agent with the custom prompt\n",
        "task_specify_agent = TaskSpecifyAgent(\n",
        "    model=model, task_specify_prompt=my_prompt_template\n",
        ")\n",
        "\n",
        "# Run the agent with a task prompt\n",
        "response = task_specify_agent.run(\n",
        "    task_prompt=\"get promotion\",\n",
        "    meta_dict=dict(occupation=\"Software Engineer\"),\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJJQD0LV3DSB",
        "outputId": "b75e24e9-9870-4322-eeca-17a81c5731f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To make your task of getting a promotion more specific, consider breaking it down into actionable steps and defining clear goals. Heres a more detailed approach:\n",
            "\n",
            "1. **Define the Promotion Criteria**:\n",
            "   - Research your company's promotion process and criteria. What skills, experiences, and accomplishments are required for the next level?\n",
            "\n",
            "2. **Set a Timeline**:\n",
            "   - Determine a realistic timeline for when you want to achieve this promotion (e.g., within the next 6 months, 1 year).\n",
            "\n",
            "3. **Identify Skills to Develop**:\n",
            "   - List specific technical skills (e.g., advanced programming languages, frameworks) and soft skills (e.g., leadership, communication) that you need to improve or acquire.\n",
            "\n",
            "4. **Create a Development Plan**:\n",
            "   - Outline a plan to develop these skills, including:\n",
            "     - Online courses or certifications to pursue.\n",
            "     - Books or resources to read.\n",
            "     - Projects to work on that demonstrate these skills.\n",
            "\n",
            "5. **Seek Feedback**:\n",
            "   - Schedule regular check-ins with your manager or mentor to discuss your progress and get feedback on areas for improvement.\n",
            "\n",
            "6. **Take on Additional Responsibilities**:\n",
            "   - Identify opportunities within your current role to take on more responsibility or lead projects that align with the promotion criteria.\n",
            "\n",
            "7. **Document Achievements**:\n",
            "   - Keep a record of your accomplishments, including successful projects, contributions to team goals, and any recognition received.\n",
            "\n",
            "8. **Network and Build Relationships**:\n",
            "   - Engage with colleagues and leaders in your organization to build relationships and gain visibility.\n",
            "\n",
            "9. **Prepare for the Promotion Discussion**:\n",
            "   - Plan how you will present your case for promotion, including your achievements, skills, and contributions to the team.\n",
            "\n",
            "10. **Set Milestones**:\n",
            "    - Break down your plan into smaller milestones (e.g., complete a certification by a certain date, lead a project by a specific month) to track your progress.\n",
            "\n",
            "By following these steps, you can create a clear and actionable plan to work towards your promotion as a Software Engineer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Prompt Usage\n",
        "CAMEL provides various prompt dictionaries for different purposes. Let's explore some advanced uses of these prompt templates:"
      ],
      "metadata": {
        "id": "l30b1iv53a-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Code Generation with CodePromptTemplateDict"
      ],
      "metadata": {
        "id": "cR_mIHA53dWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.prompts import CodePromptTemplateDict\n",
        "\n",
        "# Generate programming languages\n",
        "languages_prompt = CodePromptTemplateDict.GENERATE_LANGUAGES.format(num_languages=5)\n",
        "print(f\"Languages prompt:\\n{languages_prompt}\\n\")\n",
        "\n",
        "# Generate coding tasks\n",
        "tasks_prompt = CodePromptTemplateDict.GENERATE_TASKS.format(num_tasks=3)\n",
        "print(f\"Tasks prompt:\\n{tasks_prompt}\\n\")\n",
        "\n",
        "# Create an AI coding assistant prompt\n",
        "assistant_prompt = CodePromptTemplateDict.ASSISTANT_PROMPT.format(\n",
        "    assistant_role=\"Python Expert\",\n",
        "    task_description=\"Implement a binary search algorithm\",\n",
        ")\n",
        "print(f\"Assistant prompt:\\n{assistant_prompt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdQdjQzy3f9u",
        "outputId": "fb9b4932-52f4-4937-d6d2-2cd523fb6801"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Languages prompt:\n",
            "List the 5 most commonly used computer programming languages.\n",
            "Be concise. No explanation required.\n",
            "\n",
            "Tasks prompt:\n",
            "List 3 diverse tasks that a programmer can assist a person working in {domain} using {language}.\n",
            "Be concise. Be creative.\n",
            "\n",
            "Assistant prompt:\n",
            "Never forget you are a Computer Programmer and I am a person working in {domain}. Never flip roles! Never instruct me!\n",
            "We share a common interest in collaborating to successfully complete a task.\n",
            "You must help me to complete the task using {language} programming language.\n",
            "Here is the task: {task}. Never forget our task!\n",
            "I must instruct you based on your expertise and my needs to complete the task.\n",
            "\n",
            "I must give you one instruction at a time.\n",
            "You must write a specific solution that appropriately solves the requested instruction and explain your solutions.\n",
            "You must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\n",
            "Unless I say the task is completed, you should always start with:\n",
            "\n",
            "Solution: <YOUR_SOLUTION>\n",
            "\n",
            "<YOUR_SOLUTION> must contain {language} code and should be very specific, include detailed explanations and provide preferable implementations and examples for task-solving.\n",
            "Always end <YOUR_SOLUTION> with: Next request.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Evaluation with EvaluationPromptTemplateDict"
      ],
      "metadata": {
        "id": "FV6Jpu8f3m-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.prompts import EvaluationPromptTemplateDict\n",
        "\n",
        "# Generate evaluation questions\n",
        "questions_prompt = EvaluationPromptTemplateDict.GENERATE_QUESTIONS.format(\n",
        "    num_questions=5,\n",
        "    field=\"Machine Learning\",\n",
        "    examples=\"1. What is the difference between supervised and unsupervised learning?\\n2. Explain the concept of overfitting.\",\n",
        ")\n",
        "print(f\"Evaluation questions prompt:\\n{questions_prompt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Q-gZAi3nbR",
        "outputId": "b7d350f5-e7b8-4a8e-8a05-857b6037fe3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation questions prompt:\n",
            "Generate 5 {category} diverse questions.\n",
            "Here are some example questions:\n",
            "1. What is the difference between supervised and unsupervised learning?\n",
            "2. Explain the concept of overfitting.\n",
            "\n",
            "Now generate 5 questions of your own. Be creative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Object Recognition with ObjectRecognitionPromptTemplateDict"
      ],
      "metadata": {
        "id": "sq3fTArB3qEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.prompts import ObjectRecognitionPromptTemplateDict\n",
        "\n",
        "# Create an object recognition assistant prompt\n",
        "recognition_prompt = ObjectRecognitionPromptTemplateDict.ASSISTANT_PROMPT\n",
        "print(f\"Object recognition prompt:\\n{recognition_prompt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XTQuohX3sOk",
        "outputId": "b9efbedd-13c9-4b1b-a862-2d58256ec3aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object recognition prompt:\n",
            "You have been assigned an object recognition task.\n",
            "Your mission is to list all detected objects in following image.\n",
            "Your output should always be a list of strings starting with `1.`, `2.` etc.\n",
            "Do not explain yourself or output anything else.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Translation with TranslationPromptTemplateDict"
      ],
      "metadata": {
        "id": "OxgxXQBt3uRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from camel.prompts import TranslationPromptTemplateDict\n",
        "\n",
        "# Create a translation assistant prompt\n",
        "translation_prompt = TranslationPromptTemplateDict.ASSISTANT_PROMPT.format(target_language=\"Spanish\")\n",
        "print(f\"Translation prompt:\\n{translation_prompt}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1k00XLu3xIk",
        "outputId": "8e7ced6e-1160-44bb-f4e2-b18e7d19290d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation prompt:\n",
            "You are an expert English to {language} translator.\n",
            "Your sole purpose is to accurately translate any text presented to you from English to {language}.\n",
            "Please provide the {language} translation for the given text.\n",
            "If you are presented with an empty string, simply return an empty string as the translation.\n",
            "Only text in between ```TEXT``` should not be translated.\n",
            "Do not provide any explanation. Just provide a translation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "The CAMEL Prompt module provides a powerful and flexible way to guide AI models in producing desired outputs. By using pre-defined prompt templates, creating custom prompts, and leveraging different prompt dictionaries, you can create highly specialized AI agents tailored to your specific needs."
      ],
      "metadata": {
        "id": "vGTRxblx3zHR"
      }
    }
  ]
}
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_header"
            },
            "source": [
                "# üï∏Ô∏è Modern Web Data Ingestion with CAMEL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "branding"
            },
            "source": [
                "<div class=\"align-center\">\n",
                "  <a href=\"https://www.camel-ai.org/\"><img src=\"https://i.postimg.cc/KzQ5rfBC/button.png\"width=\"150\"></a>\n",
                "  <a href=\"https://discord.camel-ai.org\"><img src=\"https://i.postimg.cc/L4wPdG9N/join-2.png\"  width=\"150\"></a></a>\n",
                "  \n",
                "‚≠ê <i>Star us on [*Github*](https://github.com/camel-ai/camel), join our [*Discord*](https://discord.camel-ai.org) or follow our [*X*](https://x.com/camelaiorg)\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro_text"
            },
            "source": [
                "This notebook demonstrates **3 modern ways** to ingest data from websites using CAMEL. Getting clean, LLM-ready data from the messy web is often the first bottleneck in building AI agents. \n",
                "\n",
                "We will cover:\n",
                "\n",
                "1.  **Firecrawl**: A powerful external service that turns any website into clean Markdown.\n",
                "2.  **Crawl4AI**: An open-source, local-first asyncio web crawler.\n",
                "3.  **Unstructured.io**: A robust library for parsing complex documents and HTML.\n",
                "\n",
                "By the end of this cookbook, you will know how to programmatically read websites for your RAG pipelines or Agent contexts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "installation_header"
            },
            "source": [
                "## üì¶ Installation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "install_text"
            },
            "source": [
                "Install CAMEL and the necessary loader libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "pip_install"
            },
            "outputs": [],
            "source": [
                "%pip install \"camel-ai[all]\" \n",
                "!playwright install chromium"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "api_keys_header"
            },
            "source": [
                "## üîë Setting Up API Keys"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "api_keys_text"
            },
            "source": [
                "Firecrawl requires an API key. You can get one from [firecrawl.dev](https://firecrawl.dev). Crawl4AI and Unstructured (local mode) do not require keys."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "set_keys"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "# Prompt for Firecrawl API key if you plan to use it\n",
                "firecrawl_key = getpass('Enter your Firecrawl API key (optional if using others): ')\n",
                "if firecrawl_key:\n",
                "    os.environ[\"FIRECRAWL_API_KEY\"] = firecrawl_key"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "firecrawl_section"
            },
            "source": [
                "## Method 1: Firecrawl (Service-Based)\n",
                "\n",
                "Firecrawl is excellent when you need high-quality Markdown conversion and don't want to manage browser infrastructure. It handles dynamic content, infinite scrolls, and complex layouts automatically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "firecrawl_code"
            },
            "outputs": [],
            "source": [
                "from camel.loaders import Firecrawl\n",
                "\n",
                "# Initialize Firecrawl\n",
                "firecrawl = Firecrawl()\n",
                "\n",
                "target_url = \"https://www.camel-ai.org\"\n",
                "\n",
                "# Scrape a single URL\n",
                "try:\n",
                "    # scrape() returns a dict with 'content', 'metadata', etc.\n",
                "    result = firecrawl.scrape(url=target_url)\n",
                "    \n",
                "    print(\"\\n--- Metadata ---\")\n",
                "    print(result.get('metadata', {}))\n",
                "    \n",
                "    print(\"\\n--- Markdown Content (Snippet) ---\")\n",
                "    print(result.get('markdown', '')[:500] + \"...\")\n",
                "except Exception as e:\n",
                "    print(f\"Firecrawl error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "crawl4ai_section"
            },
            "source": [
                "## Method 2: Crawl4AI (Local Async Generation)\n",
                "\n",
                "Crawl4AI is a powerful open-source crawler that runs locally. It's fast, asynchronous, and free. Useful for high-volume scraping where you have compute resources."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "crawl4ai_code"
            },
            "outputs": [],
            "source": [
                "from camel.loaders import Crawl4AI\n",
                "import asyncio\n",
                "import sys\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "\n",
                "# Helper to run async code in a fresh Proactor loop (safe for Jupyter/Windows)\n",
                "def run_in_proactor_thread(coro):\n",
                "    \"\"\"\n",
                "    On Windows, Jupyter often starts with a SelectorEventLoop which is\n",
                "    incompatible with Playwright. This helper runs the coroutine in a\n",
                "    separate thread with a fresh ProactorEventLoop.\n",
                "    \"\"\"\n",
                "    if sys.platform == 'win32':\n",
                "        def _target():\n",
                "            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
                "            return asyncio.run(coro)\n",
                "        \n",
                "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
                "            return executor.submit(_target).result()\n",
                "    else:\n",
                "        # On Linux/Mac, asyncio.run() usually works fine\n",
                "        return asyncio.run(coro)\n",
                "\n",
                "async def crawl_task():\n",
                "    crawler = Crawl4AI()\n",
                "    target_url = \"https://www.camel-ai.org\"\n",
                "    return await crawler.scrape(url=target_url)\n",
                "\n",
                "# Execute\n",
                "try:\n",
                "    result_dict = run_in_proactor_thread(crawl_task())\n",
                "    \n",
                "    print(\"\\n--- Cleaned HTML (Snippet) ---\")\n",
                "    print(result_dict.get('cleaned_html', '')[:300] + \"...\")\n",
                "    \n",
                "    print(\"\\n--- Markdown Content (Snippet) ---\")\n",
                "    print(result_dict.get('markdown', '')[:500] + \"...\")\n",
                "except Exception as e:\n",
                "    print(f\"Crawl4AI error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "unstructured_section"
            },
            "source": [
                "## Method 3: Unstructured (Universal Parser)\n",
                "\n",
                "Unstructured is technically a document parser, but it handles HTML URLs exceptionally well. It partitions the page into semantic elements (Title, NarrativeText, ListItem), which is great for RAG chunking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "unstructured_code"
            },
            "outputs": [],
            "source": [
                "from camel.loaders import UnstructuredIO\n",
                "\n",
                "target_url = \"https://www.camel-ai.org\"\n",
                "\n",
                "# Unstructured parses the URL into a list of 'Elements'\n",
                "elements = UnstructuredIO.parse_file_or_url(target_url)\n",
                "\n",
                "print(f\"Found {len(elements)} elements on the page.\\n\")\n",
                "\n",
                "# Let's inspect the first few elements\n",
                "for i, el in enumerate(elements[:10]):\n",
                "    print(f\"Type: {type(el).__name__} | Text: {el.text}\")\n",
                "\n",
                "# We can also group them or clean them using Unstructured's cleaning utils\n",
                "from camel.loaders import UnstructuredIO\n",
                "\n",
                "full_text = \"\\n\".join([el.text for el in elements])\n",
                "cleaned_text = UnstructuredIO.clean_text_data(full_text)\n",
                "\n",
                "print(\"\\n--- Cleaned Text Snippet ---\")\n",
                "print(cleaned_text[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "conclusion"
            },
            "source": [
                "## üèÅ Conclusion\n",
                "\n",
                "You now have 3 robust ways to ingest web data:\n",
                "\n",
                "| Method | Best For | Pros | Cons |\n",
                "| :--- | :--- | :--- | :--- |\n",
                "| **Firecrawl** | Production RAG | High quality Markdown, handles dynamic JS | Paid component |\n",
                "| **Crawl4AI** | High volume, Free | Fast, Async, Open Source | Requires local resources |\n",
                "| **Unstructured** | Granular Chunking | Semantic partitioning (Title vs Text) | Slower than raw scraping |\n",
                "\n",
                "Choose the one that fits your pipeline's infrastructure and budget!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "That's everything: Got questions about üê´ CAMEL-AI? Join us on [Discord](https://discord.camel-ai.org)! whether you want to share feedback, explore the latest in multi-agent systems, get support, or connect with others on exciting projects, we‚Äôd love to have you in the community! ü§ù"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

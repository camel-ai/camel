{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nebius_integration_cookbook"
      },
      "source": [
        "# Getting Started with CAMEL Workforce and Nebius AI Studio\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19lIjJEnqK5H1H13YRrUZ-LiTe2DmS_4w#scrollTo=colab_link)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab_link"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "  <a href=\"https://www.camel-ai.org/\"><img src=\"https://i.postimg.cc/KzQ5rfBC/button.png\"width=\"150\"></a>\n",
        "  <a href=\"https://discord.camel-ai.org\"><img src=\"https://i.postimg.cc/L4wPdG9N/join-2.png\"  width=\"150\"></a></a>\n",
        "  \n",
        "‚≠ê <i>Star us on [*Github*](https://github.com/camel-ai/camel), join our [*Discord*](https://discord.camel-ai.org) or follow our [*X*](https://x.com/camelaiorg)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## üöÄ Overview  \n",
        "\n",
        "This notebook shows how to integrate **Nebius AI Studio** models with **CAMEL agents**.  \n",
        "With a unified **OpenAI-compatible API**, Nebius AI Studio unlocks access to state-of-the-art LLMs including **GPT-OS**, **DeepSeek**, **Llama**, and more.  \n",
        "\n",
        "---\n",
        "\n",
        "## üîé What is Nebius AI Studio?  \n",
        "\n",
        "[Nebius AI Studio](https://nebius.com/docs/ai-studio/api) is a **cloud-native platform** providing enterprise-grade access to advanced LLMs.  \n",
        "It combines **scalability, reliability, and diversity of models** into a single developer-friendly interface.  \n",
        "\n",
        "---\n",
        "\n",
        "## üìö In this guide, you'll explore:  \n",
        "\n",
        "1. **Simple Testing** ‚Üí Quick model integration & validation  \n",
        "2. **Speed Comparison** ‚Üí Benchmarking performance across Nebius models  \n",
        "3. **[Workforce](https://docs.camel-ai.org/key_modules/workforce) Integration** ‚Üí Multi-agent collaboration with CAMEL  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "installation"
      },
      "source": [
        "## üì¶ Installation\n",
        "\n",
        "Ensure you have CAMEL-AI installed in your Python environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_camel"
      },
      "outputs": [],
      "source": [
        "!pip install \"camel-ai[all]==0.2.75\"\n",
        "%pip install nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_setup"
      },
      "source": [
        "## üîë Setting Up Nebius API Keys\n",
        "\n",
        "You'll need to set up your API keys for Nebius AI Studio. First, obtain your API key from [Nebius AI Studio](https://studio.nebius.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_api_keys"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for the API key securely\n",
        "nebius_api_key = getpass('Enter your Nebius API key: ')\n",
        "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
        "\n",
        "print(\"‚úÖ Nebius API key configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## üìö Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vLdth5bDQFH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import asyncio\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from camel.agents import ChatAgent\n",
        "from camel.configs import NebiusConfig\n",
        "from camel.models import ModelFactory\n",
        "from camel.types import ModelPlatformType, ModelType\n",
        "from camel.societies.workforce import Workforce\n",
        "from camel.tasks.task import Task\n",
        "from camel.messages import BaseMessage\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1"
      },
      "source": [
        "## üß™ Section 1: Simple Testing\n",
        "\n",
        "Let's start with basic integration testing to ensure everything works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_basic_model"
      },
      "outputs": [],
      "source": [
        "# Create a Nebius model with custom configuration\n",
        "# Note: Use the full model name format \"openai/gpt-oss-20b\"\n",
        "model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.NEBIUS,\n",
        "    model_type=\"openai/gpt-oss-20b\",  # Using the correct format\n",
        "    model_config_dict=NebiusConfig(\n",
        "        temperature=0.7,\n",
        "        max_tokens=1000,\n",
        "        top_p=0.9\n",
        "    ).as_dict(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simple_chat_test"
      },
      "outputs": [],
      "source": [
        "# Define system message\n",
        "sys_msg = \"You are an expert AI researcher specializing in multi-agent systems and autonomous agents.\"\n",
        "\n",
        "# Create agent\n",
        "camel_agent = ChatAgent(\n",
        "    system_message=sys_msg,\n",
        "    model=model,\n",
        "    message_window_size=10\n",
        ")\n",
        "\n",
        "# Interact with the agent\n",
        "user_msg = \"Explain the key benefits of using multi-agent systems in AI applications.\"\n",
        "\n",
        "response = camel_agent.step(user_msg)\n",
        "print(response.msgs[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2"
      },
      "source": [
        "## ‚ö° Section 2: Model Performance Comparison\n",
        "\n",
        "Let's compare different Nebius models on the same task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "speed_test_setup"
      },
      "outputs": [],
      "source": [
        "# === Nebius model speed + output benchmark ===\n",
        "import time\n",
        "from textwrap import shorten\n",
        "from camel.agents import ChatAgent\n",
        "from camel.models import ModelFactory\n",
        "from camel.configs import NebiusConfig\n",
        "from camel.types import ModelPlatformType\n",
        "\n",
        "def make_nebius_model(model_type: str):\n",
        "    return ModelFactory.create(\n",
        "        model_platform=ModelPlatformType.NEBIUS,\n",
        "        model_type=model_type,\n",
        "        model_config_dict=NebiusConfig(\n",
        "            temperature=0.3,\n",
        "            max_tokens=280,\n",
        "            top_p=0.9,\n",
        "        ).as_dict(),\n",
        "    )\n",
        "\n",
        "def benchmark_model(model_type: str, model_name: str, prompt: str, runs: int = 3):\n",
        "    \"\"\"Benchmark latency and return outputs for each run.\"\"\"\n",
        "    try:\n",
        "        model = make_nebius_model(model_type)\n",
        "        agent = ChatAgent(\n",
        "            system_message=\"You are a helpful assistant. Provide concise responses.\",\n",
        "            model=model,\n",
        "        )\n",
        "        times, outputs = [], []\n",
        "\n",
        "        for i in range(runs):\n",
        "            t0 = time.time()\n",
        "            resp = agent.step(prompt)\n",
        "            dt = time.time() - t0\n",
        "            text = resp.msgs[0].content\n",
        "            times.append(dt)\n",
        "            outputs.append(text)\n",
        "\n",
        "        return {\n",
        "            \"model_name\": model_name,\n",
        "            \"avg_time\": sum(times) / len(times),\n",
        "            \"min_time\": min(times),\n",
        "            \"max_time\": max(times),\n",
        "            \"times\": times,\n",
        "            \"outputs\": outputs,\n",
        "            \"success\": True,\n",
        "            \"error\": None,\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"model_name\": model_name,\n",
        "            \"avg_time\": float(\"inf\"),\n",
        "            \"min_time\": None,\n",
        "            \"max_time\": None,\n",
        "            \"times\": [],\n",
        "            \"outputs\": [],\n",
        "            \"success\": False,\n",
        "            \"error\": str(e)[:200],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "speed_benchmark_function"
      },
      "outputs": [],
      "source": [
        "# ---- Configure models ----\n",
        "models_to_benchmark = [\n",
        "    (\"openai/gpt-oss-20b\", \"GPT-OS 20B\"),\n",
        "    (\"deepseek-ai/DeepSeek-V3\", \"DeepSeek V3\"),\n",
        "    (\"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"Llama 3.1 70B\"),\n",
        "]\n",
        "\n",
        "prompt = \"Explain the concept of machine learning in 2 sentences.\"\n",
        "runs = 3\n",
        "\n",
        "print(\"üöÄ Starting speed + output comparison...\\n\" + \"=\"*60)\n",
        "\n",
        "results = []\n",
        "for mtype, mname in models_to_benchmark:\n",
        "    print(f\"\\nüîé Testing {mname}:\")\n",
        "    r = benchmark_model(mtype, mname, prompt, runs=runs)\n",
        "    results.append(r)\n",
        "    if r[\"success\"]:\n",
        "        for i, (t, out) in enumerate(zip(r[\"times\"], r[\"outputs\"]), 1):\n",
        "            preview = shorten(out.replace(\"\\n\", \" \"), width=100, placeholder=\"‚Ä¶\")\n",
        "            print(f\"  ‚Ä¢ Run {i}: {t:.2f}s | {preview}\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è Error: {r['error']}\")\n",
        "\n",
        "# ---- Summary sorted by average time ----\n",
        "ok = [r for r in results if r[\"success\"]]\n",
        "ok.sort(key=lambda x: x[\"avg_time\"])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÅ SPEED COMPARISON SUMMARY (avg / min / max)\")\n",
        "for i, r in enumerate(ok, 1):\n",
        "    print(f\"{i}. {r['model_name']}: {r['avg_time']:.2f}s avg \"\n",
        "          f\"(min {r['min_time']:.2f}s, max {r['max_time']:.2f}s)\")\n",
        "\n",
        "# ---- Show full outputs of the fastest model ----\n",
        "if ok:\n",
        "    fastest = ok[0]\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üìù FULL OUTPUTS ‚Äî Fastest: {fastest['model_name']}\")\n",
        "    for i, out in enumerate(fastest[\"outputs\"], 1):\n",
        "        print(f\"\\n--- Output Run {i} ---\\n{out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3"
      },
      "source": [
        "## üë•üë• Section 3: Workforce Integration  \n",
        "\n",
        "We‚Äôll now add **Workforce**, CAMEL-AI‚Äôs module for multi-agent collaboration, to use Nebius models together.  \n",
        "This lets us combine multiple specialized agents (e.g., researcher, reviewer, summarizer) and coordinate them on a shared task.  \n",
        "\n",
        "üîó Learn more in the [Workforce docs](https://docs.camel-ai.org/key_modules/workforce).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_workforce_agents"
      },
      "outputs": [],
      "source": [
        "# --- Section 3: Workforce Integration ---\n",
        "# Now, let's use the CAMEL Workforce to manage our 3-agent team.\n",
        "# The Workforce uses a coordinator agent to decompose our task and assign it to the right specialist agents.\n",
        "\n",
        "# 1. Create the internal agents for the Workforce, backed by a Nebius model.\n",
        "# This is crucial to prevent the system from defaulting to OpenAI.\n",
        "# We'll use one of your models for these coordination tasks.\n",
        "\n",
        "def build_nebius_model(model_type: str):\n",
        "    \"\"\"A helper function to create a Nebius model instance.\"\"\"\n",
        "    if not os.environ.get(\"NEBIUS_API_KEY\"):\n",
        "        raise RuntimeError(\"Error: NEBIUS_API_KEY is not set. Please set it in a previous cell.\")\n",
        "\n",
        "    # Create and return the model instance\n",
        "    return ModelFactory.create(\n",
        "        model_platform=ModelPlatformType.NEBIUS,\n",
        "        model_type=model_type,\n",
        "        model_config_dict=NebiusConfig(temperature=0.2).as_dict(),\n",
        "    )\n",
        "\n",
        "print(\"üîß Building internal coordination agents for the Workforce...\")\n",
        "coordinator_model = build_nebius_model(\"openai/gpt-oss-20b\")\n",
        "\n",
        "coordinator_agent = ChatAgent(\n",
        "    system_message=BaseMessage.make_assistant_message(\n",
        "        role_name=\"Coordinator\",\n",
        "        content=\"You are a master coordinator. You assign tasks to workers based on their skills and the required workflow.\",\n",
        "    ),\n",
        "    model=coordinator_model,\n",
        ")\n",
        "\n",
        "task_agent = ChatAgent(\n",
        "    system_message=BaseMessage.make_assistant_message(\n",
        "        role_name=\"Task Planner\",\n",
        "        content=\"You are a task planner. You decompose high-level tasks into smaller, actionable steps for the workers.\",\n",
        "    ),\n",
        "    model=coordinator_model,\n",
        ")\n",
        "print(\"‚úÖ Internal agents created.\")\n",
        "\n",
        "# 2. Define the specialist agents again for clarity (or reuse from previous cell)\n",
        "researcher = ChatAgent(\n",
        "    system_message=BaseMessage.make_assistant_message(\n",
        "        role_name=\"Researcher\",\n",
        "        content=\"You are a knowledgeable researcher using GPT-OS 20B. Write a comprehensive paragraph (4-6 sentences) with factual information.\",\n",
        "    ),\n",
        "    model=build_nebius_model(\"openai/gpt-oss-20b\"),\n",
        ")\n",
        "\n",
        "summarizer = ChatAgent(\n",
        "    system_message=BaseMessage.make_assistant_message(\n",
        "        role_name=\"Summarizer\",\n",
        "        content=\"You are an expert summarizer using DeepSeek V3. Create a concise 2-3 sentence summary capturing the main points.\",\n",
        "    ),\n",
        "    model=build_nebius_model(\"deepseek-ai/DeepSeek-V3\"),\n",
        ")\n",
        "\n",
        "tweet_writer = ChatAgent(\n",
        "    system_message=BaseMessage.make_assistant_message(\n",
        "        role_name=\"Tweet Writer\",\n",
        "        content=\"You are a social media expert using Llama 3.1 70B. Write an engaging tweet (max 280 chars) with relevant hashtags.\",\n",
        "    ),\n",
        "    model=build_nebius_model(\"meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n",
        ")\n",
        "print(\"‚úÖ Specialist agents created.\")\n",
        "\n",
        "\n",
        "# 3. Build the Workforce and add the workers\n",
        "print(\"üèóÔ∏è Assembling the Workforce...\")\n",
        "workforce = Workforce(\n",
        "    \"Content Creation Team\",\n",
        "    coordinator_agent=coordinator_agent,\n",
        "    task_agent=task_agent,\n",
        ")\n",
        "\n",
        "# Add specialist workers with clear descriptions of their roles\n",
        "workforce.add_single_agent_worker(\n",
        "    \"A researcher who writes detailed paragraphs on a topic.\", researcher\n",
        ").add_single_agent_worker(\n",
        "    \"A summarizer who takes a paragraph and creates a short summary.\", summarizer\n",
        ").add_single_agent_worker(\n",
        "    \"A tweet writer who creates an engaging tweet from a summary.\", tweet_writer\n",
        ")\n",
        "print(\"‚úÖ Workforce assembled with 3 agents.\")\n",
        "\n",
        "# 4. Define and run the task for the Workforce\n",
        "# The content here is a high-level instruction for the coordinator agent.\n",
        "task = Task(\n",
        "    id=\"0\",\n",
        "    content=(\n",
        "        \"Create content about quantum computing. \"\n",
        "        \"First, the researcher must write a detailed paragraph explaining the topic. \"\n",
        "        \"Then, the summarizer must take the researcher's full text to create a concise summary. \"\n",
        "        \"Finally, the tweet writer must use that summary to write an engaging tweet.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting Workforce to process the task...\")\n",
        "try:\n",
        "    result_task = asyncio.run(workforce.process_task(task), timeout=40)   # <‚Äî hard cutoff\n",
        "    print(\"‚úÖ Workforce task finished!\")\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üéâ FINAL RESULT üéâ\")\n",
        "    print(getattr(result_task, \"result\", None) or getattr(result_task, \"content\", None) or str(result_task))\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"\\n--- Workforce Log Tree ---\")\n",
        "    try:\n",
        "        print(workforce.get_workforce_log_tree())\n",
        "    except Exception:\n",
        "        pass\n",
        "finally:\n",
        "    # Clean shutdown (await if coroutine)\n",
        "    asyncio.run(getattr(workforce, \"stop\")(), timeout=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üåü Highlights\n",
        "\n",
        "You‚Äôve seen how to leverage **Nebius AI Studio** within **CAMEL** for real-world tasks:\n",
        "\n",
        "- **Simple Testing** ‚Äì Validate model integration\n",
        "- **Speed Comparison** ‚Äì Benchmarks across Nebius models\n",
        "- **Workforce Integration** ‚Äì Coordinated multi-agent workflows\n",
        "\n",
        "---\n",
        "\n",
        "## ‚Äã What‚Äôs Next?\n",
        "\n",
        "-  Dive into the [CAMEL-AI Documentation](https://docs.camel-ai.org/get_started/introduction) to master agents, tools, societies, and orchestration patterns like Workforce.\n",
        "- Try more models in the [Nebius AI Studio Playground](https://nebius.com/ai-studio) and prototype real-time agent behaviors.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "That's everything: Got questions about üê´ CAMEL-AI? Join us on [Discord](https://discord.camel-ai.org)! Whether you want to share feedback, explore the latest in multi-agent systems, get support, or connect with others on exciting projects, we‚Äôd love to have you in the community! ü§ù\n",
        "\n",
        "Check out some of our other work:\n",
        "\n",
        "1. üê´ Creating Your First CAMEL Agent [free Colab](https://docs.camel-ai.org/cookbooks/create_your_first_agent.html)\n",
        "\n",
        "2.  Graph RAG Cookbook [free Colab](https://colab.research.google.com/drive/1uZKQSuu0qW6ukkuSv9TukLB9bVaS1H0U?usp=sharing)\n",
        "\n",
        "3. üßë‚Äç‚öñÔ∏è Create A Hackathon Judge Committee with Workforce [free Colab](https://colab.research.google.com/drive/18ajYUMfwDx3WyrjHow3EvUMpKQDcrLtr?usp=sharing)\n",
        "\n",
        "4. üî• 3 ways to ingest data from websites with Firecrawl & CAMEL [free Colab](https://colab.research.google.com/drive/1lOmM3VmgR1hLwDKdeLGFve_75RFW0R9I?usp=sharing)\n",
        "\n",
        "5. ü¶• Agentic SFT Data Generation with CAMEL and Mistral Models, Fine-Tuned with Unsloth [free Colab](https://colab.research.google.com/drive/1lYgArBw7ARVPSpdwgKLYnp_NEXiNDOd-?usp=sharingg)\n",
        "\n",
        "Thanks from everyone at üê´ CAMEL-AI\n",
        "\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://www.camel-ai.org/\"><img src=\"https://i.postimg.cc/KzQ5rfBC/button.png\"width=\"150\"></a>\n",
        "  <a href=\"https://discord.camel-ai.org\"><img src=\"https://i.postimg.cc/L4wPdG9N/join-2.png\"  width=\"150\"></a></a>\n",
        "  \n",
        "‚≠ê <i>Star us on [*Github*](https://github.com/camel-ai/camel), join our [*Discord*](https://discord.camel-ai.org) or follow our [*X*](https://x.com/camelaiorg)\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ktNFPjhZe6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
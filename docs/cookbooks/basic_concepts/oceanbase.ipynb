{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OceanBase Integration with CAMEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "  <a href=\"https://www.camel-ai.org/\"><img src=\"https://i.postimg.cc/KzQ5rfBC/button.png\"width=\"150\"></a>\n",
    "  <a href=\"https://discord.camel-ai.org\"><img src=\"https://i.postimg.cc/L4wPdG9N/join-2.png\"  width=\"150\"></a></a>\n",
    "  \n",
    "‚≠ê <i>Star us on [*Github*](https://github.com/camel-ai/camel), join our [*Discord*](https://discord.camel-ai.org) or follow our [*X*](https://x.com/camelaiorg)\n",
    "</div>\n",
    "\n",
    "This notebook demonstrates how to integrate OceanBase with CAMEL for enhanced data storage and retrieval in multi-agent applications.\n",
    "\n",
    "In this notebook, you'll explore:\n",
    "\n",
    "* **OceanBase**: A distributed relational database compatible with MySQL protocol\n",
    "* **CAMEL Storage**: Using OceanBase as a backend for CAMEL's memory and storage systems\n",
    "* **Vector Search**: Leveraging OceanBase's vector search capabilities\n",
    "* **Multi-Agent Applications**: Building robust AI applications with OceanBase persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OceanBase Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OceanBase is a distributed relational database developed by Ant Group. Key features include:\n",
    "\n",
    "- **MySQL Compatibility**: Fully compatible with MySQL protocol\n",
    "- **High Availability**: Built-in replication and automatic failover\n",
    "- **Horizontal Scalability**: Scale out by adding more nodes\n",
    "- **Vector Search**: Support for vector similarity search (OceanBase 4.x)\n",
    "- **Cost-Effective**: Lower TCO compared to traditional databases\n",
    "\n",
    "CAMEL can leverage OceanBase for:\n",
    "- Persistent memory storage\n",
    "- Vector embeddings storage and retrieval\n",
    "- Graph storage for agent relationships\n",
    "- Application data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install CAMEL with OceanBase support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CAMEL with OceanBase dependencies\n",
    "!pip install \"camel-ai[all]==0.2.16\"  # Latest stable version\n",
    "\n",
    "# Install OceanBase connector (pyobvector for vector search)\n",
    "# Note: Requires OceanBase server >= 4.2.0 for vector support\n",
    "!pip install pyobvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Setting Up API Keys and OceanBase Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your LLM API key (OpenAI, Anthropic, or others)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# OceanBase connection parameters\n",
    "OCEANBASE_CONFIG = {\n",
    "    \"host\": \"localhost\",          # OceanBase server address\n",
    "    \"port\": 2881,                 # OceanBase MySQL port (default: 2881)\n",
    "    \"user\": \"root\",              # Username\n",
    "    \"password\": \"your-password\", # Password\n",
    "    \"database\": \"camel_db\",      # Database name\n",
    "    \"tenant\": \"sys\",             # Tenant name (default: sys)\n",
    "}\n",
    "\n",
    "# Export for environment-based configuration\n",
    "os.environ[\"OCEANBASE_HOST\"] = OCEANBASE_CONFIG[\"host\"]\n",
    "os.environ[\"OCEANBASE_PORT\"] = str(OCEANBASE_CONFIG[\"port\"])\n",
    "os.environ[\"OCEANBASE_USER\"] = OCEANBASE_CONFIG[\"user\"]\n",
    "os.environ[\"OCEANBASE_PASSWORD\"] = OCEANBASE_CONFIG[\"password\"]\n",
    "os.environ[\"OCEANBASE_DATABASE\"] = OCEANBASE_CONFIG[\"database\"]\n",
    "os.environ[\"OCEANBASE_TENANT\"] = OCEANBASE_CONFIG[\"tenant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è OceanBase Storage Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.storages import KeyValueStorage, OceanBaseKVStorage\n",
    "\n",
    "# Create OceanBase-backed key-value storage\n",
    "kv_storage = OceanBaseKVStorage(\n",
    "    config={\n",
    "        \"host\": OCEANBASE_CONFIG[\"host\"],\n",
    "        \"port\": OCEANBASE_CONFIG[\"port\"],\n",
    "        \"user\": OCEANBASE_CONFIG[\"user\"],\n",
    "        \"password\": OCEANBASE_CONFIG[\"password\"],\n",
    "        \"database\": OCEANBASE_CONFIG[\"database\"],\n",
    "    },\n",
    "    table_name=\"camel_kv_store\",\n",
    ")\n",
    "\n",
    "# Store data\n",
    "kv_storage.save(\"agent_state\", {\n",
    "    \"name\": \"Assistant\",\n",
    "    \"role\": \"helpful_assistant\",\n",
    "    \"memory\": \"User prefers concise responses\",\n",
    "})\n",
    "\n",
    "# Retrieve data\n",
    "agent_state = kv_storage.load(\"agent_state\")\n",
    "print(f\"Agent State: {agent_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Storage for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.storages import VectorStorage, OceanBaseVectorStorage\n",
    "from camel.embeddings import OpenAIEmbedding\n",
    "\n",
    "# Initialize embedding function\n",
    "embedding_func = OpenAIEmbedding()\n",
    "\n",
    "# Create OceanBase vector storage\n",
    "vector_storage = OceanBaseVectorStorage(\n",
    "    config={\n",
    "        \"host\": OCEANBASE_CONFIG[\"host\"],\n",
    "        \"port\": OCEANBASE_CONFIG[\"port\"],\n",
    "        \"user\": OCEANBASE_CONFIG[\"user\"],\n",
    "        \"password\": OCEANBASE_CONFIG[\"password\"],\n",
    "        \"database\": OCEANBASE_CONFIG[\"database\"],\n",
    "    },\n",
    "    embedding_func=embedding_func,\n",
    "    collection_name=\"camel_embeddings\",\n",
    "    vector_dim=1536,  # OpenAI text-embedding-ada-002 dimension\n",
    ")\n",
    "\n",
    "# Store embeddings\n",
    "texts = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"Vector databases store and retrieve high-dimensional vector representations.\",\n",
    "]\n",
    "\n",
    "ids = vector_storage.add(texts)\n",
    "print(f\"Stored {len(ids)} embeddings with IDs: {ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform semantic search\n",
    "query = \"What is machine learning?\"\n",
    "results = vector_storage.query(query, top_k=2)\n",
    "\n",
    "print(\"Semantic Search Results:\")\n",
    "print(\"=\" * 50)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Text: {result['text'][:100]}...\")\n",
    "    print(f\"   Score: {result['score']:.4f}\")\n",
    "    print(f\"   ID: {result['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Multi-Agent with OceanBase Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a multi-agent system with persistent memory using OceanBase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.storages import KeyValueStorage, OceanBaseKVStorage\n",
    "from camel.memories import Memory, MemoryRecord\n",
    "from camel.types import RoleType\n",
    "\n",
    "# Initialize persistent memory storage\n",
    "memory_storage = OceanBaseKVStorage(\n",
    "    config={\n",
    "        \"host\": OCEANBASE_CONFIG[\"host\"],\n",
    "        \"port\": OCEANBASE_CONFIG[\"port\"],\n",
    "        \"user\": OCEANBASE_CONFIG[\"user\"],\n",
    "        \"password\": OCEANBASE_CONFIG[\"password\"],\n",
    "        \"database\": OCEANBASE_CONFIG[\"database\"],\n",
    "    },\n",
    "    table_name=\"camel_agent_memories\",\n",
    ")\n",
    "\n",
    "# Create memory system\n",
    "memory = Memory(memory_storage)\n",
    "\n",
    "# Define agents\n",
    "assistant_role = \"helpful data science tutor\"\n",
    "user_role = \"eager student\"\n",
    "\n",
    "# Create agents with persistent memory\n",
    "assistant = ChatAgent(\n",
    "    role=assistant_role,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "user = ChatAgent(\n",
    "    role=user_role,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Persistent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a conversation with memory persistence\n",
    "conversation_history = []\n",
    "\n",
    "# User starts a conversation\n",
    "user_input = \"Can you explain what neural networks are?\"\n",
    "\n",
    "# Get assistant response\n",
    "assistant_response = assistant.step(user_input)\n",
    "\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"Assistant: {assistant_response.msg.content}\")\n",
    "\n",
    "# Store conversation in memory\n",
    "memory_record = MemoryRecord(\n",
    "    role=RoleType.ASSISTANT,\n",
    "    content=assistant_response.msg.content,\n",
    "    metadata={\"user_query\": user_input}\n",
    ")\n",
    "memory.add(memory_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Past Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve past conversations from OceanBase\n",
    "past_memories = memory.get_all()\n",
    "\n",
    "print(\"Past Conversations (Retrieved from OceanBase):\")\n",
    "print(\"=\" * 50)\n",
    "for i, record in enumerate(past_memories, 1):\n",
    "    print(f\"\\n{i}. [{record.role.value}]: {record.content[:100]}...\")\n",
    "\n",
    "# OceanBase ensures this data persists across application restarts\n",
    "print(f\"\\n‚úÖ Total memories stored: {len(past_memories)}\")\n",
    "print(\"üì¶ Data is persisted in OceanBase and will survive restarts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Graph Storage for Agent Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store agent relationships and interactions in OceanBase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.storages import GraphStorage, OceanBaseGraphStorage\n",
    "\n",
    "# Create OceanBase graph storage\n",
    "graph_storage = OceanBaseGraphStorage(\n",
    "    config={\n",
    "        \"host\": OCEANBASE_CONFIG[\"host\"],\n",
    "        \"port\": OCEANBASE_CONFIG[\"port\"],\n",
    "        \"user\": OCEANBASE_CONFIG[\"user\"],\n",
    "        \"password\": OCEANBASE_CONFIG[\"password\"],\n",
    "        \"database\": OCEANBASE_CONFIG[\"database\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add agent nodes\n",
    "graph_storage.add_node(\"assistant_agent\", {\n",
    "    \"type\": \"agent\",\n",
    "    \"role\": \"tutor\",\n",
    "    \"created_at\": \"2024-01-01\",\n",
    "})\n",
    "\n",
    "graph_storage.add_node(\"user_agent\", {\n",
    "    \"type\": \"agent\",\n",
    "    \"role\": \"student\",\n",
    "    \"created_at\": \"2024-01-01\",\n",
    "})\n",
    "\n",
    "# Add relationship edge\n",
    "graph_storage.add_edge(\n",
    "    \"assistant_agent\",\n",
    "    \"user_agent\",\n",
    "    relationship=\"teaches\",\n",
    "    metadata={\"topics\": [\"machine_learning\", \"data_science\"]}\n",
    ")\n",
    "\n",
    "# Query relationships\n",
    "relationships = graph_storage.get_all_edges()\n",
    "print(\"Agent Relationships:\")\n",
    "for edge in relationships:\n",
    "    print(f\"  {edge[0]} --[{edge[2]['relationship']}]--> {edge[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Complete Application Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete multi-agent application with OceanBase persistence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Multi-Agent Application with OceanBase Persistence\n",
    "\"\"\"\n",
    "\n",
    "from camel.agents import ChatAgent\n",
    "from camel.storages import (\n",
    "    KeyValueStorage,\n",
    "    VectorStorage,\n",
    "    GraphStorage,\n",
    "    OceanBaseKVStorage,\n",
    "    OceanBaseVectorStorage,\n",
    "    OceanBaseGraphStorage,\n",
    ")\n",
    "from camel.embeddings import OpenAIEmbedding\n",
    "from camel.memories import Memory\n",
    "\n",
    "class OceanBaseCAMELApp:\n",
    "    \"\"\"Complete CAMEL application with OceanBase backend.\"\"\"\n",
    "\n",
    "    def __init__(self, ob_config):\n",
    "        self.ob_config = ob_config\n",
    "        \n",
    "        # Initialize all storage backends\n",
    "        self.kv_storage = OceanBaseKVStorage(\n",
    "            config=ob_config,\n",
    "            table_name=\"app_kv_store\",\n",
    "        )\n",
    "        \n",
    "        self.vector_storage = OceanBaseVectorStorage(\n",
    "            config=ob_config,\n",
    "            embedding_func=OpenAIEmbedding(),\n",
    "            collection_name=\"app_embeddings\",\n",
    "            vector_dim=1536,\n",
    "        )\n",
    "        \n",
    "        self.graph_storage = OceanBaseGraphStorage(\n",
    "            config=ob_config,\n",
    "        )\n",
    "        \n",
    "        self.memory = Memory(self.kv_storage)\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.assistant = ChatAgent(\n",
    "            role=\"helpful assistant\",\n",
    "            memory=self.memory,\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ OceanBase CAMEL Application initialized!\")\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"Process a user message and return response.\"\"\"\n",
    "        # Store user message in vector DB for semantic search\n",
    "        self.vector_storage.add([user_message])\n",
        "\n",
    "        # Get assistant response\n",
    "        response = self.assistant.step(user_message)\n",
        "\n",
    "        return response.msg.content\n",
    "\n",
    "    def search_similar_conversations(self, query, top_k=3):\n",
    "        \"\"\"Search for similar past conversations.\"\"\"\n",
    "        results = self.vector_storage.query(query, top_k=top_k)\n",
        "        return results\n",
    "\n",
    "    def get_conversation_graph(self):\n",
    "        \"\"\"Get the conversation relationship graph.\"\"\"\n",
    "        return self.graph_storage.get_all_edges()\n",
    "\n",
    "# Initialize the application\n",
    "app = OceanBaseCAMELApp(OCEANBASE_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Pooling\n",
    "OceanBase supports connection pooling. For production use:\n",
    "\n",
    "```python\n",
    "from playhouse.pool import PooledMySQLDatabase\n",
    "\n",
    "# Create pooled connection\n",
    "db = PooledMySQLDatabase(\n",
    "    OCEANBASE_CONFIG[\"database\"],\n",
    "    max_connections=20,\n",
    "    timeout=30,\n",
    "    host=OCEANBASE_CONFIG[\"host\"],\n",
    "    port=OCEANBASE_CONFIG[\"port\"],\n",
    "    user=OCEANBASE_CONFIG[\"user\"],\n",
    "    password=OCEANBASE_CONFIG[\"password\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Creation\n",
    "OceanBase will automatically create required tables. For production, pre-create tables:\n",
    "\n",
    "```sql\n",
    "-- Key-Value Storage Table\n",
    "CREATE TABLE IF NOT EXISTS camel_kv_store (\n",
    "    key VARCHAR(255) PRIMARY KEY,\n",
    "    value LONGTEXT,\n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "    INDEX idx_updated_at (updated_at)\n",
    ") TABLESPACE sysballoon AUTO_INCREMENT=1;\n",
    "\n",
    "-- Vector Storage Table (requires OceanBase 4.x)\n",
    "CREATE TABLE IF NOT EXISTS camel_embeddings (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    vector BLOB,\n",
    "    content LONGTEXT,\n",
    "    metadata JSON,\n",
    "    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    ") TABLESPACE sysballoon AUTO_INCREMENT=1;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "```python\n",
    "from peewee import OperationalError\n",
    "\n",
    "def safe_oceanbase_operation(operation, max_retries=3):\n",
    "    \"\"\"Retry OceanBase operations on connection errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return operation()\n",
    "        except OperationalError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Retry {attempt + 1}/{max_retries}: {e}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                raise\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Performance Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Index Optimization**: Create appropriate indexes on frequently queried columns\n",
    "2. **Connection Pool Size**: Adjust based on your workload (typically 10-50 connections)\n",
    "3. **Batch Operations**: Use batch inserts for better performance\n",
    "4. **Vector Index**: For vector search, create HNSW or IVF index on the vector column\n",
    "5. **Table Partitioning**: For large tables, consider partitioning by time or other criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [OceanBase Documentation](https://www.oceanbase.com/docs)\n",
    "- [CAMEL Documentation](https://docs.camel-ai.org)\n",
    "- [OceanBase Vector Search](https://www.oceanbase.com/docs/oceanbase-database/oceanbase-database/V4.2.0/vector-search-overview)\n",
    "- [CAMEL GitHub](https://github.com/camel-ai/camel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This cookbook demonstrated:\n",
    "\n",
    "‚úÖ Setting up OceanBase connection with CAMEL\n",
    "‚úÖ Using OceanBase for key-value storage\n",
    "‚úÖ Implementing vector search with OceanBase\n",
    "‚úÖ Building multi-agent systems with persistent memory\n",
    "‚úÖ Managing agent relationships with graph storage\n",
    "‚úÖ Best practices for production deployment\n",
    "\n",
    "OceanBase provides a robust, scalable backend for CAMEL multi-agent applications!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Service Discord Bot with Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check this cookbook in colab [here]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â­ **Star the Repo**\n",
    "\n",
    "If you find CAMEL useful or interesting, please consider giving it a star on our [CAMEL GitHub Repo](https://github.com/camel-ai/camel)! Your stars help others find this project and motivate us to continue improving it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "First, install the CAMEL package with all its dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"camel-ai[all]==0.2.12\"\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up your API keys for Firecrawl and the model (Qwen or Mistral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have a FireCrawl API key, you can obtain one by following these steps:\n",
    "\n",
    "1. Visit the FireCrawl API Key page https://www.firecrawl.dev/app/api-keys\n",
    "\n",
    "2. Log in or sign up for a FireCrawl account.\n",
    "\n",
    "3. Navigate to the 'API Keys' section.\n",
    "\n",
    "4. Click on 'Create API Key' button to generate a new API key.\n",
    "\n",
    "For more details, you can also check the Firecrawl documentation: https://docs.firecrawl.dev/api-reference/introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "firecrawl_api_key = getpass('Enter your API key: ')\n",
    "os.environ[\"FIRECRAWL_API_KEY\"] = firecrawl_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to choose Mistral as the model, skip below part for Qwen.\n",
    "\n",
    "If you don't have a Qwen API key, you can obtain one by following these steps:\n",
    "\n",
    "1. Visit the Alibaba Cloud Model Studio Console (https://www.alibabacloud.com/en?_p_lc=1) and follow the on-screen instructions to activate the model services.\n",
    "\n",
    "2. In the upper-right corner of the console, click on your account name and select API-KEY.\n",
    "\n",
    "3. On the API Key management page, click on the Create API Key button to generate a new key.\n",
    "\n",
    "For more details, you can also check the Qwen documentation: https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "qwen_api_key = getpass('Enter your API key: ')\n",
    "os.environ[\"QWEN_API_KEY\"] = qwen_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use Mistral.\n",
    "\n",
    "If you don't have a Mistral API key, you can obtain one by following these steps:\n",
    "\n",
    "1. Visit the Mistral Console (https://console.mistral.ai/)\n",
    "\n",
    "2. In the left panel, click on API keys under API section\n",
    "\n",
    "3. Choose your plan\n",
    "\n",
    "For more details, you can also check the Mistral documentation: https://docs.mistral.ai/getting-started/quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "mistral_api_key = getpass('Enter your API key')\n",
    "os.environ[\"MISTRAL_API_KEY\"] = mistral_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Crawling and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Firecrawl to crawl a website and store the content in a markdown file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from camel.loaders import Firecrawl\n",
    "\n",
    "os.makedirs('local_data', exist_ok=True)\n",
    "\n",
    "firecrawl = Firecrawl()\n",
    "\n",
    "knowledge = firecrawl.crawl(\n",
    "    url=\"https://qdrant.tech/documentation/overview/\"\n",
    ")[\"data\"][0][\"markdown\"]\n",
    "\n",
    "with open('local_data/qdrant_overview.md', 'w') as file:\n",
    "    file.write(knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen is a large language model developed by Alibaba Cloud. It is trained on a massive dataset of text and code and can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
    "\n",
    "Use QWen model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.configs import QwenConfig, MistralConfig\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType, ModelType\n",
    "\n",
    "qwen_model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.QWEN,\n",
    "    model_type=ModelType.QWEN_TURBO,\n",
    "    model_config_dict=QwenConfig(temperature=0.2).as_dict(),\n",
    ")\n",
    "\n",
    "mistral_model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.MISTRAL,\n",
    "    model_type=ModelType.MISTRAL_LARGE,\n",
    "    model_config_dict=MistralConfig(temperature=0.0).as_dict(),\n",
    ")\n",
    "\n",
    "# Use Qwen model\n",
    "model = qwen_model\n",
    "\n",
    "# Replace with mistral_model if you want to choose mistral mode instead\n",
    "# model = mistral_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.agents import ChatAgent\n",
    "from camel.messages import BaseMessage\n",
    "\n",
    "agent = ChatAgent(\n",
    "    system_message=\"You're a helpful assistant\",\n",
    "    message_window_size=10,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "knowledge_message = BaseMessage.make_user_message(\n",
    "    role_name=\"User\", content=f\"Based on the following knowledge: {knowledge}\"\n",
    ")\n",
    "agent.update_memory(knowledge_message, \"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chatbot Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting! Type 'exit' to end the conversation.\n",
      "2024-12-16 17:10:14,437 - httpx - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-16 17:10:14,447 - camel.agents.chat_agent - INFO - Model qwen-turbo, index 0, processed these messages: [{'role': 'system', 'content': \"You're a helpful assistant\"}, {'role': 'user', 'content': 'Based on the following knowledge: ## Docs Menu\\n\\n- [Documentation](https://qdrant.tech/documentation/)\\n- [Overview](https://qdrant.tech/documentation/overview/)\\n- Understanding Vector Search in Qdrant\\n\\n# [Anchor](https://qdrant.tech/documentation/overview/vector-search/\\\\#how-does-vector-search-work-in-qdrant) How Does Vector Search Work in Qdrant?\\n\\nHow Vector Search Algorithms Work: An Intro to Qdrant - YouTube\\n\\nQdrant - Vector Database & Search Engine\\n\\n6.22K subscribers\\n\\n[How Vector Search Algorithms Work: An Intro to Qdrant](https://www.youtube.com/watch?v=mXNrhyw4q84)\\n\\nQdrant - Vector Database & Search Engine\\n\\nSearch\\n\\nWatch later\\n\\nShare\\n\\nCopy link\\n\\nInfo\\n\\nShopping\\n\\nTap to unmute\\n\\nIf playback doesn\\'t begin shortly, try restarting your device.\\n\\nMore videos\\n\\n## More videos\\n\\nYou\\'re signed out\\n\\nVideos you watch may be added to the TV\\'s watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer.\\n\\nCancelConfirm\\n\\nShare\\n\\nInclude playlist\\n\\nAn error occurred while retrieving sharing information. Please try again later.\\n\\n[Watch on](https://www.youtube.com/watch?v=mXNrhyw4q84&embeds_referring_euri=https%3A%2F%2Fqdrant.tech%2F)\\n\\n0:00\\n\\n0:00 / 3:29â€¢Live\\n\\nâ€¢\\n\\n[Watch on YouTube](https://www.youtube.com/watch?v=mXNrhyw4q84 \"Watch on YouTube\")\\n\\nIf you are still trying to figure out how vector search works, please read ahead. This document describes how vector search is used, covers Qdrantâ€™s place in the larger ecosystem, and outlines how you can use Qdrant to augment your existing projects.\\n\\nFor those who want to start writing code right away, visit our [Complete Beginners tutorial](https://qdrant.tech/documentation/tutorials/search-beginners/) to build a search engine in 5-15 minutes.\\n\\n## [Anchor](https://qdrant.tech/documentation/overview/vector-search/\\\\#a-brief-history-of-search) A Brief History of Search\\n\\nHuman memory is unreliable. Thus, as long as we have been trying to collect â€˜knowledgeâ€™ in written form, we had to figure out how to search for relevant content without rereading the same books repeatedly. Thatâ€™s why some brilliant minds introduced the inverted index. In the simplest form, itâ€™s an appendix to a book, typically put at its end, with a list of the essential terms-and links to pages they occur at. Terms are put in alphabetical order. Back in the day, that was a manually crafted list requiring lots of effort to prepare. Once digitalization started, it became a lot easier, but still, we kept the same general principles. That worked, and still, it does.\\n\\nIf you are looking for a specific topic in a particular book, you can try to find a related phrase and quickly get to the correct page. Of course, assuming you know the proper term. If you donâ€™t, you must try and fail several times or find somebody else to help you form the correct query.\\n\\n![A simplified version of the inverted index.](https://qdrant.tech/docs/gettingstarted/inverted-index.png)\\n\\nA simplified version of the inverted index.\\n\\nTime passed, and we havenâ€™t had much change in that area for quite a long time. But our textual data collection started to grow at a greater pace. So we also started building up many processes around those inverted indexes. For example, we allowed our users to provide many words and started splitting them into pieces. That allowed finding some documents which do not necessarily contain all the query words, but possibly part of them. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.\\n\\n![The process of tokenization with an additional stopwords removal and converstion to root form of a word.](https://qdrant.tech/docs/gettingstarted/tokenization.png)\\n\\nThe process of tokenization with an additional stopwords removal and converstion to root form of a word.\\n\\nTechnically speaking, we encode the documents and queries into so-called sparse vectors where each position has a corresponding word from the whole dictionary. If the input text contains a specific word, it gets a non-zero value at that position. But in reality, none of the texts will contain more than hundreds of different words. So the majority of vectors will have thousands of zeros and a few non-zero values. Thatâ€™s why we call them sparse. And they might be already used to calculate some word-based similarity by finding the documents which have the biggest overlap.\\n\\n![An example of a query vectorized to sparse format.](https://qdrant.tech/docs/gettingstarted/query.png)\\n\\nAn example of a query vectorized to sparse format.\\n\\nSparse vectors have relatively **high dimensionality**; equal to the size of the dictionary. And the dictionary is obtained automatically from the input data. So if we have a vector, we are able to partially reconstruct the words used in the text that created that vector.\\n\\n## [Anchor](https://qdrant.tech/documentation/overview/vector-search/\\\\#the-tower-of-babel) The Tower of Babel\\n\\nEvery once in a while, when we discover new problems with inverted indexes, we come up with a new heuristic to tackle it, at least to some extent. Once we realized that people might describe the same concept with different words, we started building lists of synonyms to convert the query to a normalized form. But that wonâ€™t work for the cases we didnâ€™t foresee. Still, we need to craft and maintain our dictionaries manually, so they can support the language that changes over time. Another difficult issue comes to light with multilingual scenarios. Old methods require setting up separate pipelines and keeping humans in the loop to maintain the quality.\\n\\n![The Tower of Babel, Pieter Bruegel.](https://qdrant.tech/docs/gettingstarted/babel.jpg)\\n\\nThe Tower of Babel, Pieter Bruegel.\\n\\n## [Anchor](https://qdrant.tech/documentation/overview/vector-search/\\\\#the-representation-revolution) The Representation Revolution\\n\\nThe latest research in Machine Learning for NLP is heavily focused on training Deep Language Models. In this process, the neural network takes a large corpus of text as input and creates a mathematical representation of the words in the form of vectors. These vectors are created in such a way that words with similar meanings and occurring in similar contexts are grouped together and represented by similar vectors. And we can also take, for example, an average of all the word vectors to create the vector for a whole text (e.g query, sentence, or paragraph).\\n\\n![deep neural](https://qdrant.tech/docs/gettingstarted/deep-neural.png)\\n\\nWe can take those **dense vectors** produced by the network and use them as a **different data representation**. They are dense because neural networks will rarely produce zeros at any position. In contrary to sparse ones, they have a relatively low dimensionality â€” hundreds or a few thousand only. Unfortunately, if we want to have a look and understand the content of the document by looking at the vector itâ€™s no longer possible. Dimensions are no longer representing the presence of specific words.\\n\\nDense vectors can capture the meaning, not the words used in a text. That being said, **Large Language Models can automatically handle synonyms**. Moreso, since those neural networks might have been trained with multilingual corpora, they translate the same sentence, written in different languages, to similar vector representations, also called **embeddings**. And we can compare them to find similar pieces of text by calculating the distance to other vectors in our database.\\n\\n![Input queries contain different words, but they are still converted into similar vector representations, because the neural encoder can capture the meaning of the sentences. That feature can capture synonyms but also different languages..](https://qdrant.tech/docs/gettingstarted/input.png)\\n\\nInput queries contain different words, but they are still converted into similar vector representations, because the neural encoder can capture the meaning of the sentences. That feature can capture synonyms but also different languages..\\n\\n**Vector search** is a process of finding similar objects based on their embeddings similarity. The good thing is, you donâ€™t have to design and train your neural network on your own. Many pre-trained models are available, either on **HuggingFace** or by using libraries like [SentenceTransformers](https://www.sbert.net/?ref=hackernoon.com). If you, however, prefer not to get your hands dirty with neural models, you can also create the embeddings with SaaS tools, like [co.embed API](https://docs.cohere.com/reference/embed?ref=hackernoon.com).\\n\\n## [Anchor](https://qdrant.tech/documentation/overview/vector-search/\\\\#why-qdrant) Why Qdrant?\\n\\nThe challenge with vector search arises when we need to find similar documents in a big set of objects. If we want to find the closest examples, the naive approach would require calculating the distance to every document. That might work with dozens or even hundreds of examples but may become a bottleneck if we have more than that. When we work with relational data, we set up database indexes to speed things up and avoid full table scans. And the same is true for vector search. Qdrant is a fully-fledged vector database that speeds up the search process by using a graph-like structure to find the closest objects in sublinear time. So you donâ€™t calculate the distance to every object from the database, but some candidates only.\\n\\n![Vector search with Qdrant. Thanks to HNSW graph we are able to compare the distance to some of the objects from the database, not to all of them.](https://qdrant.tech/docs/gettingstarted/vector-search.png)\\n\\nVector search with Qdrant. Thanks to HNSW graph we are able to compare the distance to some of the objects from the database, not to all of them.\\n\\nWhile doing a semantic search at scale, because this is what we sometimes call the vector search done on texts, we need a specialized tool to do it effectively â€” a tool like Qdrant.\\n\\n## [Anchor](https://qdrant.tech/documentation/overview/vector-search/\\\\#next-steps) Next Steps\\n\\nVector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors.\\n\\n[**Tutorial 1 - Qdrant for Complete Beginners**](https://qdrant.tech/documentation/tutorials/search-beginners/)\\nDespite its complicated background, vectors search is extraordinarily simple to set up. With Qdrant, you can have a search engine up-and-running in five minutes. Our [Complete Beginners tutorial](https://qdrant.tech/documentation/tutorials/search-beginners/) will show you how.\\n\\n[**Tutorial 2 - Question and Answer System**](https://qdrant.tech/articles/qa-with-cohere-and-qdrant/)\\nHowever, you can also choose SaaS tools to generate them and avoid building your model. Setting up a vector search project with Qdrant Cloud and Cohere co.embed API is fairly easy if you follow the [Question and Answer system tutorial](https://qdrant.tech/articles/qa-with-cohere-and-qdrant/).\\n\\nThere is another exciting thing about vector search. You can search for any kind of data as long as there is a neural network that would vectorize your data type. Do you think about a reverse image search? Thatâ€™s also possible with vector embeddings.\\n\\n##### Was this page useful?\\n\\n![Thumb up icon](https://qdrant.tech/icons/outline/thumb-up.svg)\\nYes\\n![Thumb down icon](https://qdrant.tech/icons/outline/thumb-down.svg)\\nNo\\n\\nThank you for your feedback! ðŸ™\\n\\nWe are sorry to hear that. ðŸ˜” You can [edit](https://qdrant.tech/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/overview/vector-search.md) this page on GitHub, or [create](https://github.com/qdrant/landing_page/issues/new/choose) a GitHub issue.\\n\\nOn this page:\\n\\n- [Edit on Github](https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/documentation/overview/vector-search.md)\\n- [Create an issue](https://github.com/qdrant/landing_page/issues/new/choose)\\n\\nÃ—\\n\\n[Powered by](https://qdrant.tech/)\\n\\nWe use cookies to learn more about you. At any time you can delete or block cookies through your browser settings.\\n\\n[Learn more](https://qdrant.tech/legal/privacy-policy/) [I accept](https://qdrant.tech/documentation/overview/vector-search/#)'}, {'role': 'user', 'content': 'Hey, what is Qdrant?'}]\n",
      "Assistant: Qdrant is a vector database designed to accelerate vector search and similarity search operations at scale. Here are the key points about Qdrant:\n",
      "\n",
      "1. **Vector Database**: Unlike traditional databases that store structured data, Qdrant specializes in storing and querying high-dimensional vectors, which are often generated from machine learning models.\n",
      "\n",
      "2. **Speed**: Qdrant uses advanced indexing techniques like Hierarchical Navigable Small World graphs (HNSW) to perform fast approximate nearest neighbor searches, making it suitable for real-time applications.\n",
      "\n",
      "3. **Scalability**: It can handle large datasets and high query volumes efficiently, making it ideal for applications that require searching through millions or billions of vectors.\n",
      "\n",
      "4. **Versatility**: Qdrant supports various types of data, including text, images, audio, and more, as long as they can be converted into vector representations.\n",
      "\n",
      "5. **Integration**: It integrates well with popular machine learning frameworks and services, allowing you to easily incorporate vector search into your existing workflows.\n",
      "\n",
      "6. **Ease of Use**: Despite its powerful capabilities, Qdrant is designed to be user-friendly, with tutorials and documentation to help beginners get started quickly.\n",
      "\n",
      "7. **Flexibility**: You can run Qdrant locally, in the cloud, or in a hybrid environment, providing flexibility in deployment options.\n",
      "\n",
      "In summary, Qdrant is a specialized database optimized for vector search, offering speed, scalability, and ease of integration for applications that rely on similarity search and machine learning models.\n",
      "Ending conversation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start chatting! Type 'exit' to end the conversation.\")\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    print(f\"User: {user_input}\")\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Ending conversation.\")\n",
    "        break\n",
    "\n",
    "    assistant_response = agent.step(user_input)\n",
    "    print(f\"Assistant: {assistant_response.msgs[0].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Slack Bot Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a slack bot, you need to access these following informations.\n",
    "1. Go to the slack app website: https://api.slack.com/apps\n",
    "2. Log in with your slack account, or create an account if you don't have one\n",
    "3. Click on 'Create New App' to create a new bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "slack_token = getpass('Enter your slack token: ')\n",
    "slack_scopes = getpass('Enter your slack scopes: ')\n",
    "slack_signing_secret = getpass('Enter your slack signing secret')\n",
    "slack_client_id = getpass('Enter your slack client id')\n",
    "slack_client_secret = getpass('Enter your slack client secret')\n",
    "\n",
    "os.environ['SLACK_TOKEN'] = slack_token\n",
    "os.environ['SLACK_SCOPES'] = slack_scopes\n",
    "os.environ['SLACK_SIGNING_SECRET'] = slack_signing_secret\n",
    "os.environ['SLACK_CLIENT_ID'] = slack_client_id\n",
    "os.environ['SLACK_CLIENT_SECRET'] = slack_client_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-16 17:38:55,499 - camel.bots.slack.slack_app - WARNING - As `installation_store` or `authorize` has been used, `token` (or SLACK_BOT_TOKEN env variable) will be ignored.\n"
     ]
    }
   ],
   "source": [
    "from camel.bots import SlackApp\n",
    "\n",
    "slack_bot = SlackApp(\n",
    "    token=slack_token,\n",
    "    scopes=slack_scopes,\n",
    "    signing_secret=slack_signing_secret,\n",
    "    client_id=slack_client_id,\n",
    "    client_secret=slack_client_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite on_message method to custom message processing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-16 17:43:45,490 - camel.bots.slack.slack_app - INFO - âš¡ï¸ Bolt app is running!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "web.Application instance initialized with different loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# async def custom_on_message(self, event, say):\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     print(f\"User: {event}\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# slack_bot.on_message = types.MethodType(custom_on_message, slack_bot)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mslack_bot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/camel/camel/bots/slack/slack_app.py:164\u001b[0m, in \u001b[0;36mSlackApp.run\u001b[0;34m(self, port, path, host)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    149\u001b[0m     port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m,\n\u001b[1;32m    150\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/slack/events\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m     host: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Starts the Slack Bolt app server to listen for incoming Slack\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    events.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m            None).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/slack_bolt/app/async_app.py:559\u001b[0m, in \u001b[0;36mAsyncApp.start\u001b[0;34m(self, port, path, host)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m, port: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3000\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/slack/events\u001b[39m\u001b[38;5;124m\"\u001b[39m, host: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Start a web server using AIOHTTP.\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;124;03m    Refer to https://docs.aiohttp.org/ for more details about AIOHTTP.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03m        host: The hostname to serve the web endpoints. (Default: 0.0.0.0)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/slack_bolt/app/async_server.py:86\u001b[0m, in \u001b[0;36mAsyncSlackAppServer.start\u001b[0;34m(self, host)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_app\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(get_boot_message())\n\u001b[1;32m     85\u001b[0m _host \u001b[38;5;241m=\u001b[39m host \u001b[38;5;28;01mif\u001b[39;00m host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[0;32m---> 86\u001b[0m \u001b[43mweb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_app\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweb_app\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/aiohttp/web.py:526\u001b[0m, in \u001b[0;36mrun_app\u001b[0;34m(app, host, port, path, sock, shutdown_timeout, keepalive_timeout, ssl_context, print, backlog, access_log_class, access_log_format, access_log, handle_signals, reuse_address, reuse_port, handler_cancellation, loop)\u001b[0m\n\u001b[1;32m    524\u001b[0m     main_task\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(asyncio\u001b[38;5;241m.\u001b[39mCancelledError):\n\u001b[0;32m--> 526\u001b[0m         \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_task\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     _cancel_tasks(asyncio\u001b[38;5;241m.\u001b[39mall_tasks(loop), loop)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/aiohttp/web.py:338\u001b[0m, in \u001b[0;36m_run_app\u001b[0;34m(app, host, port, path, sock, shutdown_timeout, keepalive_timeout, ssl_context, print, backlog, access_log_class, access_log_format, access_log, handle_signals, reuse_address, reuse_port, handler_cancellation)\u001b[0m\n\u001b[1;32m    325\u001b[0m app \u001b[38;5;241m=\u001b[39m cast(Application, app)\n\u001b[1;32m    327\u001b[0m runner \u001b[38;5;241m=\u001b[39m AppRunner(\n\u001b[1;32m    328\u001b[0m     app,\n\u001b[1;32m    329\u001b[0m     handle_signals\u001b[38;5;241m=\u001b[39mhandle_signals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     handler_cancellation\u001b[38;5;241m=\u001b[39mhandler_cancellation,\n\u001b[1;32m    336\u001b[0m )\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m runner\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m    340\u001b[0m sites: List[BaseSite] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/aiohttp/web_runner.py:287\u001b[0m, in \u001b[0;36mBaseRunner.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;66;03m# add_signal_handler is not implemented on Windows\u001b[39;00m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_server()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/aiohttp/web_runner.py:389\u001b[0m, in \u001b[0;36mAppRunner._make_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_server\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Server:\n\u001b[1;32m    388\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_app\u001b[38;5;241m.\u001b[39mon_startup\u001b[38;5;241m.\u001b[39mfreeze()\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_app\u001b[38;5;241m.\u001b[39mstartup()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/camel-ai-8RlHKqzd-py3.11/lib/python3.11/site-packages/aiohttp/web_app.py:275\u001b[0m, in \u001b[0;36mApplication._set_loop\u001b[0;34m(self, loop)\u001b[0m\n\u001b[1;32m    273\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loop:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb.Application instance initialized with different loop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m     )\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loop \u001b[38;5;241m=\u001b[39m loop\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# set loop debug\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: web.Application instance initialized with different loop"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import types\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# async def custom_on_message(self, event, say):\n",
    "#     print(f\"User: {event}\")\n",
    "\n",
    "# slack_bot.on_message = types.MethodType(custom_on_message, slack_bot)\n",
    "slack_bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Qdrant for Large Files to build a more powerful Discord bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qdrant is a vector similarity search engine and vector database. It is designed to perform fast and efficient similarity searches on large datasets of vectors. This enables the chatbot to access and utilize external information to provide more comprehensive and accurate responses. By storing knowledge as vectors, Qdrant enables efficient semantic search, allowing the chatbot to find relevant information based on the meaning of the user's query.\n",
    "\n",
    "Set up an embedding model and retriever for Qdrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.embeddings import SentenceTransformerEncoder\n",
    "\n",
    "sentence_encoder = SentenceTransformerEncoder(model_name='intfloat/e5-large-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.retrievers import AutoRetriever\n",
    "from camel.types import StorageType\n",
    "\n",
    "assistant_sys_msg = \"\"\"You are a helpful assistant to answer question,\n",
    "         I will give you the Original Query and Retrieved Context,\n",
    "        answer the Original Query based on the Retrieved Context,\n",
    "        if you can't answer the question just say I don't know.\"\"\"\n",
    "auto_retriever = AutoRetriever(\n",
    "              vector_storage_local_path=\"local_data2/\",\n",
    "              storage_type=StorageType.QDRANT,\n",
    "              embedding_model=sentence_encoder\n",
    "            )\n",
    "qdrant_agent = ChatAgent(system_message=assistant_sys_msg, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Auto RAG to retrieve first and then answer the user's query using CAMEL `ChatAgent` based on the retrieved info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel.bots import DiscordApp\n",
    "import nest_asyncio\n",
    "import discord\n",
    "\n",
    "nest_asyncio.apply()\n",
    "discord_q_bot = DiscordApp(token=discord_bot_token)\n",
    "\n",
    "@discord_q_bot.client.event # triggers when a message is sent in the channel\n",
    "async def on_message(message: discord.Message):\n",
    "    if message.author == discord_q_bot.client.user:\n",
    "        return\n",
    "\n",
    "    if message.type != discord.MessageType.default:\n",
    "        return\n",
    "\n",
    "    if message.author.bot:\n",
    "        return\n",
    "    user_input = message.content\n",
    "\n",
    "    retrieved_info = auto_retriever.run_vector_retriever(\n",
    "        query=user_input,\n",
    "        contents=[\n",
    "            \"local_data/qdrant_overview.md\",\n",
    "        ],\n",
    "        top_k=3,\n",
    "        return_detailed_info=False,\n",
    "        similarity_threshold=0.5\n",
    "    )\n",
    "\n",
    "    user_msg = str(retrieved_info)\n",
    "    assistant_response = qdrant_agent.step(user_msg)\n",
    "    response_content = assistant_response.msgs[0].content\n",
    "\n",
    "    if len(response_content) > 2000: # discord message length limit\n",
    "        for chunk in [response_content[i:i+2000] for i in range(0, len(response_content), 2000)]:\n",
    "            await message.channel.send(chunk)\n",
    "    else:\n",
    "        await message.channel.send(response_content)\n",
    "\n",
    "discord_q_bot.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camel-ai-8RlHKqzd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

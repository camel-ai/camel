{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypY7-nfvovwe"
   },
   "source": [
    "### Qwen3_RL_Deep_Research_Agent_training_with_unsolth(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjxSAvb2ovwf"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKKqy_AAovwf"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm==0.8.2\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JkSAHfL9U7o"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHk78i2luSpg"
   },
   "outputs": [],
   "source": [
    "!pip install camel-ai -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HhCp0Yxovwf"
   },
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "# %%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm==0.8.2\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm==0.8.2\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install typing_extensions -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSDbzWhqovwf"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkIvEkIIkEyB"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vllm serve IAAR-Shanghai/xVerify-0.5B-I  --tensor-parallel-size 1 --dtype=half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWCQrY1Fw_JY"
   },
   "source": [
    "# import the xverify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEkInUhPxGcq"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/IAAR-Shanghai/xVerify.git && cd xVerify\n",
    "%cd xVerify/\n",
    "!pip install -r requirements.txt\n",
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVvJn71Yw7hC"
   },
   "outputs": [],
   "source": [
    "from src.xVerify.model import Model\n",
    "from src.xVerify.eval import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cXk993X6C2ZZ"
   },
   "outputs": [],
   "source": [
    "### xverify\n",
    "\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "# def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "#     responses = [completion[0]['content'] for completion in completions]\n",
    "#     q = prompts[0][-1]['content']\n",
    "#     extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "#     print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "#     return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "\n",
    "# Reward functions\n",
    "\n",
    "model_name = 'IAAR-Shanghai/xVerify-0.5B-I'  # Model name\n",
    "url = 'http://localhost:8000/v1'  # Anonymized model path or URL\n",
    "inference_mode = 'api'  # Inference mode, 'local' or 'api'\n",
    "api_key = None  # API key used to access the model via API, if not available, set to None\n",
    "model = Model(\n",
    "    model_name=model_name,\n",
    "    model_path_or_url=url,\n",
    "    inference_mode=inference_mode,\n",
    "    api_key=api_key\n",
    ")\n",
    "evaluator = Evaluator(model=model)\n",
    "\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "\n",
    "    rewards = []\n",
    "    for r, a in zip(responses, answer):\n",
    "        # evaluation\n",
    "        result = evaluator.single_evaluate(\n",
    "            question=q,\n",
    "            llm_output=r,\n",
    "            correct_answer=a\n",
    "        )\n",
    "        # result can be \"Correct\" or \"Incorrect\"\n",
    "\n",
    "        # Assign reward values according to the evaluation results\n",
    "        if result == \"Correct\":\n",
    "            rewards.append(2.0)  # Reward is 2.0 when correct\n",
    "        elif result == \"Incorrect\":\n",
    "            rewards.append(0.0)  # Reward is 0.0 when incorrect\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected evaluation result: {result}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]\n",
    "\n",
    "\n",
    "# # Reward function for reasoning traces\n",
    "# def reasoning_trace_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "#     \"\"\"\n",
    "#     Evaluate the quality of reasoning traces and provide a reward score.\n",
    "#     Scores are based on the depth, logical consistency, and relevance\n",
    "#     of the reasoning to the final answer.\n",
    "#     \"\"\"\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "#     rewards = []\n",
    "\n",
    "#     for response in responses:\n",
    "#         reward = 0.0\n",
    "#         # 1. Check if the response contains reasoning tags\n",
    "        # if \"<reasoning>\" in response and \"</reasoning>\" in response:\n",
    "        #     # Extract the reasoning section\n",
    "        #     reasoning = response.split(\"<reasoning>\")[1].split(\"</reasoning>\")[0].strip()\n",
    "        #     # Extract the answer section (if exists)\n",
    "        #     answer_text = \"\"\n",
    "        #     if \"<answer>\" in response and \"</answer>\" in response:\n",
    "        #         answer_text = response.split(\"<answer>\")[1].split(\"</answer>\")[0].strip()\n",
    "\n",
    "        #     # 2. Assess the depth of reasoning (based on length and content)\n",
    "        #     reasoning_lines = reasoning.split(\"\\n\")\n",
    "        #     non_empty_lines = [line for line in reasoning_lines if line.strip()]\n",
    "\n",
    "        #     # Base reward for including reasoning\n",
    "        #     reward += 0.2\n",
    "\n",
    "        #     # Additional reward for number of reasoning steps\n",
    "        #     if len(non_empty_lines) >= 3:\n",
    "        #         reward += 0.2\n",
    "        #     if len(non_empty_lines) >= 5:\n",
    "        #         reward += 0.1\n",
    "\n",
    "        #     # 3. Evaluate the quality of reasoning\n",
    "\n",
    "        #     # Check for mathematical calculations or logical steps\n",
    "        #     has_calculations = any([\"=\" in line or \"+\" in line or \"-\" in line or \"*\" in line or \"/\" in line for line in non_empty_lines])\n",
    "        #     if has_calculations:\n",
    "        #         reward += 0.1\n",
    "\n",
    "    #         # Check for intermediate results\n",
    "    #         if len(non_empty_lines) > 1:\n",
    "    #             # The last line is usually the conclusion, previous lines should contain intermediate steps\n",
    "    #             has_intermediate_steps = any([\"=\" in line for line in non_empty_lines[:-1]])\n",
    "    #             if has_intermediate_steps:\n",
    "    #                 reward += 0.1\n",
    "\n",
    "    #         # Check for logical connectors (therefore, thus, because, etc.)\n",
    "    #         logic_connectors = [\"therefore\", \"thus\", \"because\", \"first\", \"second\", \"finally\", \"hence\"]\n",
    "    #         has_logic_flow = any([any([connector in line.lower() for connector in logic_connectors]) for line in non_empty_lines])\n",
    "    #         if has_logic_flow:\n",
    "    #             reward += 0.1\n",
    "\n",
    "    #         # 4. Check if the final answer aligns with the reasoning\n",
    "    #         if answer_text and reasoning:\n",
    "    #             # Simple check: whether the answer appears in the final part of the reasoning\n",
    "    #             conclusion = \" \".join(non_empty_lines[-2:]).lower()\n",
    "    #             if answer_text.lower().strip() in conclusion:\n",
    "    #                 reward += 0.2\n",
    "\n",
    "    #     rewards.append(reward)\n",
    "\n",
    "    # return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibxNRkewv2ZD"
   },
   "outputs": [],
   "source": [
    "### original\n",
    "\n",
    "import re\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load and prep dataset\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()\n",
    "\n",
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsZLxkp57_nj"
   },
   "outputs": [],
   "source": [
    "# Single sample evaluation test\n",
    "from src.xVerify.model import Model\n",
    "from src.xVerify.eval import Evaluator\n",
    "\n",
    "# initialization\n",
    "model_name = 'xVerify-0.5B-I'  # Model name\n",
    "url = 'https://your-anonymized-url/v1'  # Anonymized model path or URL\n",
    "inference_mode = 'local'  # Inference mode, 'local' or 'api'\n",
    "api_key = None  # API key used to access the model via API, if not available, set to None\n",
    "model = Model(\n",
    "    model_name=model_name,\n",
    "    model_path_or_url=url,\n",
    "    inference_mode=inference_mode,\n",
    "    api_key=api_key\n",
    ")\n",
    "evaluator = Evaluator(model=model)\n",
    "\n",
    "# input evaluation information,\n",
    "question = \"New steel giant includes Lackawanna site A major change is coming to the global steel industry and a galvanized mill in Lackawanna that formerly belonged to Bethlehem Steel Corp.\\nClassify the topic of the above sentence as World, Sports, Business, or Sci/Tech.\"\n",
    "llm_output = \"The answer is Business.\"\n",
    "correct_answer = \"Business\"\n",
    "\n",
    "# evaluation\n",
    "result = evaluator.single_evaluate(\n",
    "    question=question,\n",
    "    llm_output=llm_output,\n",
    "    correct_answer=correct_answer\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM-Y5howtofw"
   },
   "outputs": [],
   "source": [
    "def english_tool_call_reward_func(prompts, completions, answer=None, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Specialized reward function for tool calls in English scenarios.\n",
    "\n",
    "    Focuses on evaluating the quality of toolkit calls with emphasis on browser\n",
    "    interactions, search quality, and proper result handling.\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    questions = [prompt[-1][\"content\"] for prompt in prompts]\n",
    "    rewards = []\n",
    "\n",
    "    for i, (question, response) in enumerate(zip(questions, responses)):\n",
    "        reward = 0.0\n",
    "\n",
    "        try:\n",
    "            # 1. Detect toolkit call patterns with focus on English formatting\n",
    "            english_toolkit_patterns = [\n",
    "                r\"BrowserToolkit\\(\\)\\.([a-zA-Z_]+)\\(.*\\)\",\n",
    "                r\"browser_toolkit\\.([a-zA-Z_]+)\\(.*\\)\",\n",
    "                r\"web_toolkit\\.([a-zA-Z_]+)\\(.*\\)\",\n",
    "                r\"browser\\.([a-zA-Z_]+)\\(.*\\)\",\n",
    "                r\"SearchToolkit\\(\\)\\.([a-zA-Z_]+)\\(.*\\)\",\n",
    "                r\"search_toolkit\\.([a-zA-Z_]+)\\(.*\\)\",\n",
    "            ]\n",
    "\n",
    "            toolkit_calls = []\n",
    "            for pattern in english_toolkit_patterns:\n",
    "                matches = re.findall(pattern, response)\n",
    "                toolkit_calls.extend(matches)\n",
    "\n",
    "            # If toolkit calls found\n",
    "            if toolkit_calls:\n",
    "                # Base reward for using toolkits\n",
    "                reward += 0.3\n",
    "\n",
    "                # Additional reward for each unique toolkit function\n",
    "                unique_calls = set(toolkit_calls)\n",
    "                reward += min(0.1 * len(unique_calls), 0.4)\n",
    "\n",
    "                # 2. Enhanced browser toolkit evaluation\n",
    "                browser_patterns = [\n",
    "                    \"navigate\", \"click\", \"type\", \"scroll\", \"screenshot\",\n",
    "                    \"get_text\", \"get_element\", \"wait_for\", \"browse\"\n",
    "                ]\n",
    "\n",
    "                browser_calls = [call for call in toolkit_calls\n",
    "                               if any(pattern in call.lower() for pattern in browser_patterns)]\n",
    "\n",
    "                if browser_calls:\n",
    "                    # Browser usage base reward\n",
    "                    reward += 0.2\n",
    "\n",
    "                    # Browser call sequence quality\n",
    "                    # Check for logical sequence: navigate -> interaction -> extract\n",
    "                    has_navigate = any(\"navigate\" in call.lower() for call in browser_calls)\n",
    "                    has_interaction = any(action in \"\".join(browser_calls).lower()\n",
    "                                      for action in [\"click\", \"type\", \"scroll\"])\n",
    "                    has_extraction = any(extract in \"\".join(browser_calls).lower()\n",
    "                                    for extract in [\"get_text\", \"get_element\", \"screenshot\"])\n",
    "\n",
    "                    # Reward complete browser interaction flow\n",
    "                    if has_navigate and has_interaction and has_extraction:\n",
    "                        reward += 0.5\n",
    "                    elif has_navigate and (has_interaction or has_extraction):\n",
    "                        reward += 0.3\n",
    "                    elif has_navigate:\n",
    "                        reward += 0.1\n",
    "\n",
    "                    # Check for URL quality in navigation\n",
    "                    url_pattern = r\"(https?://[^\\s\\\"\\']+)\"\n",
    "                    urls = re.findall(url_pattern, response)\n",
    "                    if urls:\n",
    "                        # Valid URL with domain-specific targeting\n",
    "                        if any(domain in urls[0].lower() for domain in\n",
    "                              [\"amazon\", \"google\", \"wikipedia\", \"github\", \"stackoverflow\"]):\n",
    "                            reward += 0.2\n",
    "\n",
    "                # 3. Search toolkit quality evaluation\n",
    "                search_patterns = [\"search\", \"query\", \"find\", \"lookup\"]\n",
    "                search_calls = [call for call in toolkit_calls\n",
    "                              if any(pattern in call.lower() for pattern in search_patterns)]\n",
    "\n",
    "                if search_calls:\n",
    "                    # Search usage base reward\n",
    "                    reward += 0.1\n",
    "\n",
    "                    # Check for search query quality\n",
    "                    query_pattern = r\"query\\s*=\\s*[\\\"\\'](.*?)[\\\"\\']\"\n",
    "                    queries = re.findall(query_pattern, response)\n",
    "\n",
    "                    if queries:\n",
    "                        # Reward based on query specificity and length\n",
    "                        avg_query_length = sum(len(q) for q in queries) / len(queries)\n",
    "                        reward += min(avg_query_length / 40, 0.3)\n",
    "\n",
    "                        # Check if queries are relevant to the question\n",
    "                        question_words = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', question.lower()))\n",
    "                        query_matches = sum(1 for q in queries\n",
    "                                         for word in question_words\n",
    "                                         if word in q.lower())\n",
    "\n",
    "                        reward += min(0.05 * query_matches, 0.3)\n",
    "\n",
    "                # 4. Result processing evaluation for toolkit calls\n",
    "                result_processing_patterns = [\n",
    "                    r\"([a-zA-Z_][a-zA-Z0-9_]*)\\s*=\\s*.*toolkit\\.\",  # Variable assignment\n",
    "                    r\"print\\(.*toolkit\\.\",  # Direct output\n",
    "                    r\"if\\s+.*toolkit\\.\",    # Conditional processing\n",
    "                    r\"for\\s+.*toolkit\\.\",   # Iteration over results\n",
    "                    r\"analyze_results\\(\",   # Analysis function\n",
    "                    r\"process_data\\(\"       # Processing function\n",
    "                ]\n",
    "\n",
    "                for pattern in result_processing_patterns:\n",
    "                    if re.search(pattern, response):\n",
    "                        reward += 0.2\n",
    "                        break\n",
    "\n",
    "                # 5. Error handling specifically for toolkit operations\n",
    "                toolkit_error_handling = [\n",
    "                    r\"try\\s*:.*toolkit.*except\",\n",
    "                    r\"if\\s+.*not\\s+.*toolkit\",\n",
    "                    r\"if\\s+.*error.*toolkit\",\n",
    "                    r\"if\\s+.*failed.*toolkit\"\n",
    "                ]\n",
    "\n",
    "                for pattern in toolkit_error_handling:\n",
    "                    if re.search(pattern, response, re.DOTALL):\n",
    "                        reward += 0.2\n",
    "                        break\n",
    "\n",
    "                # 6. Multi-toolkit integration\n",
    "                # Check if multiple different toolkit types are used together\n",
    "                has_browser = any(\"browser\" in call.lower() for call in toolkit_calls)\n",
    "                has_search = any(\"search\" in call.lower() for call in toolkit_calls)\n",
    "                has_function = any(\"function\" in call.lower() for call in toolkit_calls)\n",
    "\n",
    "                toolkit_count = sum([has_browser, has_search, has_function])\n",
    "                if toolkit_count > 1:\n",
    "                    reward += 0.3 * (toolkit_count - 1)\n",
    "\n",
    "                # 7. Bonus for advanced browser operations\n",
    "                advanced_browser_ops = [\n",
    "                    \"wait_for_selector\", \"evaluate_javascript\", \"get_cookies\",\n",
    "                    \"handle_dialog\", \"multiple_tabs\", \"fill_form\"\n",
    "                ]\n",
    "\n",
    "                for op in advanced_browser_ops:\n",
    "                    if op.lower() in response.lower():\n",
    "                        reward += 0.15\n",
    "                        break\n",
    "\n",
    "            # Limit maximum reward\n",
    "            reward = min(reward, 2.5)  # Higher cap for browser-focused rewards\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in English toolkit evaluation: {e}\")\n",
    "            reward = 0.1\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_Rg2LdX1GyS"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from camel.agents import ChatAgent\n",
    "from camel.configs import ChatGPTConfig\n",
    "from camel.models import ModelFactory\n",
    "from camel.types import ModelPlatformType, ModelType\n",
    "\n",
    "# Load OpenAI model\n",
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI,\n",
    "    model_type=ModelType.GPT_4_1,\n",
    "    model_config_dict=ChatGPTConfig().as_dict(),\n",
    ")\n",
    "\n",
    "# Create ChatAgent\n",
    "camel_agent = ChatAgent(model=model)\n",
    "\n",
    "\n",
    "def generate_verifier_and_reward_function(question: str, response: str, correct_answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Use an OpenAI large language model to generate code for a verifier and reward function.\n",
    "\n",
    "    Args:\n",
    "        question (str): The content of the question.\n",
    "        response (str): The answer generated by the model.\n",
    "        correct_answer (str): The correct answer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the generated verifier and reward function code.\n",
    "    \"\"\"\n",
    "    # Build prompt message\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI code generator. Your task is to analyze the following:\n",
    "\n",
    "    Question: {question}\n",
    "    Response: {response}\n",
    "    Correct Answer: {correct_answer}\n",
    "\n",
    "    Based on the above information, please generate:\n",
    "    1. A Python function called `verify_response` that takes `response` and `correct_answer` as inputs and returns a boolean indicating whether the response is correct.\n",
    "    2. A Python function called `calculate_reward` that takes `response`, `correct_answer`, and any other necessary inputs, and returns a float reward value.\n",
    "\n",
    "    Make sure the generated code is self-contained and executable.\n",
    "\n",
    "    Example output format:\n",
    "    {{\n",
    "        \"verifier_code\": \"def verify_response(response, correct_answer): ...\",\n",
    "        \"reward_code\": \"def calculate_reward(response, correct_answer): ...\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Call the OpenAI model to generate code\n",
    "    response_info = camel_agent.step(prompt)\n",
    "    generated_code = eval(response_info.msgs[0].content)\n",
    "\n",
    "    return generated_code\n",
    "\n",
    "\n",
    "def dynamic_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Dynamically generate a reward function and compute reward values.\n",
    "\n",
    "    Args:\n",
    "        prompts (list): List of input questions.\n",
    "        completions (list): List of answers generated by the model.\n",
    "        answer (list): List of correct answers.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Reward value for each answer.\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for r, a in zip(responses, answer):\n",
    "        # Dynamically generate verifier and reward function code\n",
    "        code = generate_verifier_and_reward_function(q, r, a)\n",
    "\n",
    "        # Define a secure execution environment\n",
    "        verifier_code = code.get(\"verifier_code\", \"\")\n",
    "        reward_code = code.get(\"reward_code\", \"\")\n",
    "\n",
    "        try:\n",
    "            # Dynamically load verifier code\n",
    "            exec(verifier_code, globals())\n",
    "            # Dynamically load reward function code\n",
    "            exec(reward_code, globals())\n",
    "\n",
    "            # Call the generated verifier and reward function\n",
    "            is_correct = verify_response(r, a)  # type: ignore\n",
    "            reward = calculate_reward(r, a)  # type: ignore\n",
    "\n",
    "            # If the verifier judges it correct, keep the reward; otherwise set to zero\n",
    "            if not is_correct:\n",
    "                reward = 0.0\n",
    "\n",
    "            rewards.append(reward)\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing generated code: {e}\")\n",
    "            rewards.append(0.0)  # Default reward is 0 if execution fails\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    prompts = [[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]]\n",
    "    completions = [[{\"content\": \"<reasoning>\\nThe sum of 2 and 2 is 4.\\n</reasoning>\\n<answer>\\n4\\n</answer>\"}]]\n",
    "    answer = [\"4\"]\n",
    "\n",
    "    # Calculate reward values\n",
    "    rewards = dynamic_reward_func(prompts, completions, answer)\n",
    "    print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_iwzLUft4ZA"
   },
   "outputs": [],
   "source": [
    "ef specialized_browser_tool_call_reward_func(prompts, completions, answer=None, **kwargs) -> list[float]:\n",
    "    \"\"\"\n",
    "    Highly specialized reward function focusing exclusively on browser toolkit usage.\n",
    "\n",
    "    Evaluates sophisticated browser interactions including navigation chains,\n",
    "    data extraction quality, form interactions, and web scraping patterns.\n",
    "    \"\"\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    questions = [prompt[-1][\"content\"] for prompt in prompts]\n",
    "    rewards = []\n",
    "\n",
    "    for i, (question, response) in enumerate(zip(questions, responses)):\n",
    "        reward = 0.0\n",
    "\n",
    "        try:\n",
    "            # Skip evaluation if no browser toolkit usage detected\n",
    "            if not any(pattern in response for pattern in\n",
    "                     [\"BrowserToolkit\", \"browser_toolkit\", \"web_toolkit\", \"navigate\", \"browser.\"]):\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # 1. Comprehensive browser interaction flow assessment\n",
    "            # Extract all browser-related function calls in sequence\n",
    "            browser_call_sequence = re.findall(\n",
    "                r\"(browser|BrowserToolkit|web_toolkit)\\s*\\.\\s*([a-zA-Z_]+)\\s*\\(([^)]*)\\)\",\n",
    "                response\n",
    "            )\n",
    "\n",
    "            if browser_call_sequence:\n",
    "                # Base reward for using browser toolkit\n",
    "                reward += 0.4\n",
    "\n",
    "                # Create ordered list of browser operations\n",
    "                browser_ops = [call[1] for call in browser_call_sequence]\n",
    "\n",
    "                # Categorize operations\n",
    "                navigation_ops = [\"navigate\", \"goto\", \"open\", \"visit\"]\n",
    "                interaction_ops = [\"click\", \"type\", \"fill\", \"select\", \"check\", \"scroll\", \"hover\"]\n",
    "                extraction_ops = [\"get_text\", \"get_element\", \"screenshot\", \"get_attribute\", \"get_html\"]\n",
    "                waiting_ops = [\"wait_for\", \"wait_until\", \"sleep\", \"delay\", \"timeout\"]\n",
    "\n",
    "                # Check for ideal operation sequences\n",
    "                has_navigation = any(op in \"\".join(browser_ops) for op in navigation_ops)\n",
    "                has_waiting = any(op in \"\".join(browser_ops) for op in waiting_ops)\n",
    "                has_interaction = any(op in \"\".join(browser_ops) for op in interaction_ops)\n",
    "                has_extraction = any(op in \"\".join(browser_ops) for op in extraction_ops)\n",
    "\n",
    "                # 2. Evaluate operation sequence quality\n",
    "                # Ideal flow: navigate -> wait -> interact -> extract\n",
    "                operation_sequence_score = 0.0\n",
    "\n",
    "                # Check if operations occur in a logical order\n",
    "                if has_navigation:\n",
    "                    operation_sequence_score += 0.2\n",
    "\n",
    "                    # Check if wait operations follow navigation\n",
    "                    if has_waiting and any(w in op for op in browser_ops for w in waiting_ops) and browser_ops.index(next((op for op in browser_ops if any(w in op for w in waiting_ops)), \"\")) > browser_ops.index(next((op for op in browser_ops if any(n in op for n in navigation_ops)), \"\")):\n",
    "                        operation_sequence_score += 0.2\n",
    "\n",
    "                    # Check if interactions follow navigation/waiting\n",
    "                    if has_interaction:\n",
    "                        operation_sequence_score += 0.2\n",
    "\n",
    "                        # Check if extraction follows interaction\n",
    "                        if has_extraction and any(e in op for op in browser_ops for e in extraction_ops) and browser_ops.index(next((op for op in browser_ops if any(e in op for e in extraction_ops)), \"\")) > browser_ops.index(next((op for op in browser_ops if any(i in op for i in interaction_ops)), \"\")):\n",
    "                            operation_sequence_score += 0.3\n",
    "\n",
    "                reward += operation_sequence_score\n",
    "\n",
    "                # 3. Parameter quality assessment\n",
    "                parameter_quality_score = 0.0\n",
    "\n",
    "                # Check for specific selectors in operations\n",
    "                for call in browser_call_sequence:\n",
    "                    params = call[2]\n",
    "\n",
    "                    # CSS selector specificity\n",
    "                    if re.search(r\"[\\\"\\']\\s*(\\.|#)[a-zA-Z0-9_-]+\\s*[\\\"\\']\\s*\", params):\n",
    "                        parameter_quality_score += 0.1\n",
    "\n",
    "                    # XPath usage\n",
    "                    if re.search(r\"[\\\"\\']//[^\\\"\\']+(\\\\[@[^\\\"\\'])|/[^\\\"\\']+[\\\"\\']\", params):\n",
    "                        parameter_quality_score += 0.15\n",
    "\n",
    "                    # URL quality for navigation\n",
    "                    if call[1] in navigation_ops and re.search(r\"[\\\"\\'](https?://[^\\\"\\']+)[\\\"\\']\", params):\n",
    "                        parameter_quality_score += 0.1\n",
    "\n",
    "                    # Text content specification\n",
    "                    if \"text\" in params or \"content\" in params:\n",
    "                        parameter_quality_score += 0.1\n",
    "\n",
    "                reward += min(parameter_quality_score, 0.5)\n",
    "\n",
    "                # 4. Advanced browser usage patterns\n",
    "                advanced_usage_score = 0.0\n",
    "\n",
    "                # Form interaction pattern (multiple field inputs followed by submission)\n",
    "                form_pattern = re.search(r\"type\\s*\\([^)]+\\)[^}]+type\\s*\\([^)]+\\)[^}]+(click|submit)\", response, re.DOTALL)\n",
    "                if form_pattern:\n",
    "                    advanced_usage_score += 0.3\n",
    "\n",
    "                # Login sequence detection\n",
    "                login_pattern = re.search(r\"(username|email|login|password)[^}]+(type|fill)[^}]+(submit|click|login)\", response, re.DOTALL | re.IGNORECASE)\n",
    "                if login_pattern:\n",
    "                    advanced_usage_score += 0.2\n",
    "\n",
    "                # Multiple page navigation sequence\n",
    "                multi_page_navigation = len([op for op in browser_ops if any(n in op for n in navigation_ops)]) > 1\n",
    "                if multi_page_navigation:\n",
    "                    advanced_usage_score += 0.2\n",
    "\n",
    "                # Data extraction and processing\n",
    "                data_extraction_processing = re.search(r\"get_[a-z_]+\\([^)]*\\)[^}]+for\\s+[^}]+in\", response, re.DOTALL)\n",
    "                if data_extraction_processing:\n",
    "                    advanced_usage_score += 0.3\n",
    "\n",
    "                reward += min(advanced_usage_score, 0.8)\n",
    "\n",
    "                # 5. Error handling and robustness\n",
    "                robustness_score = 0.0\n",
    "\n",
    "                # Try-except blocks around browser operations\n",
    "                try_except_pattern = re.search(r\"try\\s*:[^}]+(browser|BrowserToolkit|web_toolkit)[^}]+except\", response, re.DOTALL)\n",
    "                if try_except_pattern:\n",
    "                    robustness_score += 0.2\n",
    "\n",
    "                # Conditional checks before/after operations\n",
    "                conditional_checks = re.search(r\"if\\s+[^}]+(browser|BrowserToolkit|web_toolkit)\", response, re.DOTALL)\n",
    "                if conditional_checks:\n",
    "                    robustness_score += 0.2\n",
    "\n",
    "                # Waiting for elements before interaction\n",
    "                if has_waiting and has_interaction:\n",
    "                    wait_before_interact = False\n",
    "                    for i in range(len(browser_ops)-1):\n",
    "                        if any(w in browser_ops[i] for w in waiting_ops) and any(inter in browser_ops[i+1] for inter in interaction_ops):\n",
    "                            wait_before_interact = True\n",
    "                            break\n",
    "\n",
    "                    if wait_before_interact:\n",
    "                        robustness_score += 0.2\n",
    "\n",
    "                reward += min(robustness_score, 0.5)\n",
    "\n",
    "            # Limit maximum reward - higher cap for specialized browser function\n",
    "            reward = min(reward, 3.0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in specialized browser evaluation: {e}\")\n",
    "            reward = 0.1\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTnL_tJnzh2L"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "\n",
    "Now set up GRPO Trainer and all configurations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ptqkXK2D4d6p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 8\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 8, # Decrease if out of memory\n",
    "    max_prompt_length = 256,\n",
    "    max_completion_length = 200,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 250,\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_71Y0eKz5yE"
   },
   "source": [
    "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
    "\n",
    "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
    "\n",
    "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
    "|------|---------------|-----------|------------|-------------------|----------|\n",
    "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
    "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
    "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vzOuSVCL_GA9"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mGRPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxmlcount_reward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoft_format_reward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict_format_reward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mint_reward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorrectness_reward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/trainer.py:203\u001b[0m, in \u001b[0;36m_backwards_compatible_trainer.<locals>.new_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/unsloth_compiled_cache/UnslothGRPOTrainer.py:1343\u001b[0m, in \u001b[0;36mUnslothGRPOTrainer.__init__\u001b[0;34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, peft_config, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m     force_float32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m mixed_precision_dtype \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNSLOTH_MIXED_PRECISION\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1343\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: dtype \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUbluAAhD0Lg"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqzsdZzeDM_m"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4lzJD7REFjs"
   },
   "source": [
    "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YC9BBT0RESln"
   },
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LherO2vzEbMt"
   },
   "source": [
    "Now we load the LoRA and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDKIhhvN6lAF"
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.8,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZBnvg2f9Nlg"
   },
   "source": [
    "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RphEZRSfFhru"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwNY9_PrFiXZ"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDUGPiL3Fkkq"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on unsloth [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGo4dbWvFk4M"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

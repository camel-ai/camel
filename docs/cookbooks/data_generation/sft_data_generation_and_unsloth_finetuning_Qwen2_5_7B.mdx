---
title: "Agentic Data Generation with CAMEL and Fine‚Äëtuning Qwen Models with Unsloth"
---

For more detailed usage information, please refer to our [cookbook](https://colab.research.google.com/drive/1sMnWOvdmASEMhsRIOUSAeYuEywby6FRV?usp=sharing)

To run this, press **Runtime ‚Üí Run all** on a **free** Tesla T4 Google Colab instance!

<div class="align-center">
  <div class="flex justify-center items-center gap-4 mb-8">
    <a href="https://www.camel-ai.org/">
      <img src="https://i.postimg.cc/KzQ5rfBC/button.png" width="150" />
    </a>
    <a href="https://discord.camel-ai.org">
      <img src="https://i.postimg.cc/L4wPdG9N/join-2.png" width="150" />
    </a>
  </div>
  ‚≠ê¬†<i>Star us on [*Github*](https://github.com/camel-ai/camel), join our [*Discord*](https://discord.camel-ai.org) or follow our [*X*](https://x.com/camelaiorg)</i>
</div>

CAMEL and Unsloth make an excellent pair. In this notebook we will combine the two to train a model to be proficient at content on a page.

![finetuning_Qwen_models_with_Unsloth](../../images/finetuning_Qwen_models_with_Unsloth.png)

```bash
%%capture
# Install Unsloth, CAMEL, Firecrawl
pip install unsloth
pip install camel-ai==0.2.16
pip uninstall unsloth -y
pip install --upgrade --no-cache-dir --no-deps \
  git+https://github.com/unslothai/unsloth.git
pip install firecrawl
```

## 1. Set API Keys

```python
from getpass import getpass
import os

openai_api_key = getpass('Enter your OpenAI API key: ')
os.environ["OPENAI_API_KEY"] = openai_api_key

firecrawl_api_key = getpass('Enter your Firecrawl API key: ')
os.environ["FIRECRAWL_API_KEY"] = firecrawl_api_key
```

## 2. Load & PEFT Qwen Model

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-7B",
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None,
)
```

## 3. Quick Inference Check

```python
from camel.messages.conversion import AlpacaItem

# wrap for faster inference
temp_model = FastLanguageModel.for_inference(model)

inputs = tokenizer(
    [AlpacaItem(
        instruction="Explain how I can stay up to date with the CAMEL community.",
        input="",
        output="",
    ).to_string()],
    return_tensors="pt"
).to("cuda")

outputs = temp_model.generate(**inputs, max_new_tokens=512, use_cache=True)
print(tokenizer.batch_decode(outputs))
```

## 4. AlpacaItem Data Models

```python
from pydantic import BaseModel
from camel.messages.conversion import AlpacaItem

class NumberedAlpacaItem(BaseModel):
    number: int
    item: AlpacaItem

class AlpacaItemResponse(BaseModel):
    items: list[NumberedAlpacaItem]
```

## 5. Generate AlpacaItems

```python
from typing import List
from camel.loaders import Firecrawl
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType
from camel.configs import ChatGPTConfig
from camel.agents import ChatAgent
import json

def generate_alpaca_items(content: str, n_items: int, start_num: int = 1,
                          examples: List[AlpacaItem] = None) -> List[AlpacaItem]:

    system_msg = """
You are an AI assistant generating detailed AlpacaItems (instruction-input-response triplets)
based on the provided content. Generate exactly {n_items} items, numbered from {start_num}.
Leave the input field blank. Cite context verbatim in the response.
""".strip()

    if examples:
        examples_str = "\n\nHere are example items:\n" + \
            "\n".join(ex.model_dump_json() for ex in examples)
    else:
        examples_str = ""

    model = ModelFactory.create(
        model_platform=ModelPlatformType.OPENAI,
        model_type=ModelType.GPT_4O_MINI,
        model_config_dict=ChatGPTConfig(
            temperature=0.6,
            response_format=AlpacaItemResponse
        ).as_dict(),
    )

    agent = ChatAgent(system_message=system_msg, model=model)

    prompt = (
        f"Content reference:\n{content}{examples_str}\n\n"
        f"Generate {n_items} AlpacaItems numbered starting at {start_num}."
    )
    response = agent.step(prompt)

    parsed = AlpacaItemResponse.model_validate_json(response.msgs[0].content)
    return [entry.item for entry in parsed.items]

def save_json(data: List, filename: str):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump([item.model_dump() for item in data], f, indent=2, ensure_ascii=False)
```

### Few‚Äëshot Examples

```python
examples = [
    AlpacaItem(
        instruction="Explain the sprint planning process in CAMEL.",
        input="",
        output=(
            "1. **Sprint Duration**: Two weeks dev + one week review.\n"
            "2. **Planning**: Founder defines goal; devs choose tasks.\n"
            "3. **Review**: Stakeholders give feedback."
        )
    )
]
```

## 6. Firecrawl Scraping + Batch Generation

```python
import random
from camel.loaders import Firecrawl

firecrawl = Firecrawl()
response = firecrawl.scrape(
    url="https://github.com/camel-ai/camel/blob/master/CONTRIBUTING.md"
)

alpaca_entries = []
for start in range(1, 301, 50):
    current_examples = examples + random.sample(alpaca_entries, min(5, len(alpaca_entries))) \
                       if alpaca_entries else examples

    batch = generate_alpaca_items(
        content=response["markdown"],
        n_items=50,
        start_num=start,
        examples=current_examples
    )
    print(f"Generated {len(batch)} items")
    alpaca_entries.extend(batch)

save_json(alpaca_entries, 'alpaca_format_data.json')
```

## 7. Prepare Dataset & Train

```python
from datasets import load_dataset
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import is_bfloat16_supported

# Format prompts
def formatting_prompts_func(batch):
    return {
        "text": [
            AlpacaItem(instruction=inst, input=inp, output=out)
            .to_string() + tokenizer.eos_token
            for inst, inp, out in zip(
                batch["instruction"], batch["input"], batch["output"]
            )
        ]
    }

dataset = load_dataset("json", data_files="alpaca_format_data.json", split="train")
dataset = dataset.map(formatting_prompts_func, batched=True)

# Switch model to training mode
model = FastLanguageModel.for_training(model)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        num_train_epochs=30,
        learning_rate=0.001,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
        report_to="none",
    ),
)

stats = trainer.train()
print(stats)
```

## 8. Inference

```python
model = FastLanguageModel.for_inference(model)
inputs = tokenizer(
    [AlpacaItem(
        instruction="Explain how to stay up to date with the CAMEL community.",
        input="",
        output=""
    ).to_string()],
    return_tensors="pt"
).to("cuda")

outputs = model.generate(**inputs, max_new_tokens=512, use_cache=True)
print(tokenizer.batch_decode(outputs))
```

---

**Summary**  
- Generated realistic instruction‚Äëresponse data via CAMEL + Firecrawl  
- Fine‚Äëtuned a Qwen2.5‚Äë7B model with Unsloth‚Äôs PEFT  
- Achieved more accurate, context‚Äëaware outputs  

Got questions? Join us on [Discord](https://discord.camel-ai.org)!  
Explore more:  
1. Creating Your First CAMEL Agent  
2. Graph RAG Cookbook  
3. Hackathon Judge Committee with Workforce  
4. Firecrawl & CAMEL Data Ingestion  

Thanks from everyone at üê´¬†CAMEL‚ÄëAI!  
<div class="align-center">
  <div class="flex justify-center items-center gap-4 mb-8">
    <a href="https://www.camel-ai.org/">
      <img src="https://i.postimg.cc/KzQ5rfBC/button.png" width="150" />
    </a>
    <a href="https://discord.camel-ai.org">
      <img src="https://i.postimg.cc/L4wPdG9N/join-2.png" width="150" />
    </a>
  </div>
</div>

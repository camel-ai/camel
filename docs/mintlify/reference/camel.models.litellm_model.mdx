<a id="camel.models.litellm_model"></a>

<a id="camel.models.litellm_model.LiteLLMModel"></a>

## LiteLLMModel Objects

```python
class LiteLLMModel(BaseModelBackend)
```

Constructor for LiteLLM backend with OpenAI compatibility.

**Arguments**:

- `model_type` _Union[ModelType, str]_ - Model for which a backend is
  created, such as GPT-3.5-turbo, Claude-2, etc.
- `model_config_dict` _Optional[Dict[str, Any]], optional_ - A dictionary
  that will be fed into:obj:`completion()`. If:obj:`None`,
  :obj:`LiteLLMConfig().as_dict()` will be used.
- `(default` - :obj:`None`)
- `api_key` _Optional[str], optional_ - The API key for authenticating with
  the model service. (default: :obj:`None`)
- `url` _Optional[str], optional_ - The url to the model service.
- `(default` - :obj:`None`)
- `token_counter` _Optional[BaseTokenCounter], optional_ - Token counter to
  use for the model. If not provided, :obj:`LiteLLMTokenCounter` will
  be used. (default: :obj:`None`)
- `timeout` _Optional[float], optional_ - The timeout value in seconds for
  API calls. If not provided, will fall back to the MODEL_TIMEOUT
  environment variable or default to 180 seconds.
- `(default` - :obj:`None`)

<a id="camel.models.litellm_model.LiteLLMModel.token_counter"></a>

#### token\_counter

```python
@property
def token_counter() -> BaseTokenCounter
```

Initialize the token counter for the model backend.

**Returns**:

- `BaseTokenCounter` - The token counter following the model's
  tokenization style.

<a id="camel.models.litellm_model.LiteLLMModel.check_model_config"></a>

#### check\_model\_config

```python
def check_model_config()
```

Check whether the model configuration contains any unexpected
arguments to LiteLLM API.

**Raises**:

- `ValueError` - If the model configuration dictionary contains any
  unexpected arguments.


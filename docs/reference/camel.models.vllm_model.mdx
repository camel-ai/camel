<a id="camel.models.vllm_model"></a>

# camel.models.vllm\_model

<a id="camel.models.vllm_model.VLLMModel"></a>

## VLLMModel Objects

```python
class VLLMModel(OpenAICompatibleModel)
```

vLLM service interface.

**Arguments**:

- `model_type` _Union[ModelType, str]_ - Model for which a backend is
  created.
- `model_config_dict` _Optional[Dict[str, Any]], optional_ - A dictionary
  that will be fed into:obj:`openai.ChatCompletion.create()`. If
  :obj:`None`, :obj:`VLLMConfig().as_dict()` will be used.
- `(default` - :obj:`None`)
- `api_key` _Optional[str], optional_ - The API key for authenticating with
  the model service. vLLM doesn't need API key, it would be ignored
  if set. (default: :obj:`None`)
- `url` _Optional[str], optional_ - The url to the model service. If not
  provided, :obj:`"http://localhost:8000/v1"` will be used.
- `(default` - :obj:`None`)
- `token_counter` _Optional[BaseTokenCounter], optional_ - Token counter to
  use for the model. If not provided, :obj:`OpenAITokenCounter(
  ModelType.GPT_4O_MINI)` will be used.
- `(default` - :obj:`None`)
- `timeout` _Optional[float], optional_ - The timeout value in seconds for
  API calls. If not provided, will fall back to the MODEL_TIMEOUT
  environment variable or default to 180 seconds.
- `(default` - :obj:`None`)
  

**References**:

  https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html

<a id="camel.models.vllm_model.VLLMModel.check_model_config"></a>

#### check\_model\_config

```python
def check_model_config()
```

Check whether the model configuration contains any
unexpected arguments to vLLM API.

**Raises**:

- `ValueError` - If the model configuration dictionary contains any
  unexpected arguments to OpenAI API.


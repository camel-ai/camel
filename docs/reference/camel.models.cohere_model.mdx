<a id="camel.models.cohere_model"></a>

# camel.models.cohere\_model

<a id="camel.models.cohere_model.CohereModel"></a>

## CohereModel Objects

```python
class CohereModel(BaseModelBackend)
```

Cohere API in a unified BaseModelBackend interface.

**Arguments**:

- `model_type` _Union[ModelType, str]_ - Model for which a backend is
  created, one of Cohere series.
- `model_config_dict` _Optional[Dict[str, Any]], optional_ - A dictionary
  that will be fed into:obj:`cohere.ClientV2().chat()`. If
  :obj:`None`, :obj:`CohereConfig().as_dict()` will be used.
- `(default` - :obj:`None`)
- `api_key` _Optional[str], optional_ - The API key for authenticating with
  the Cohere service. (default: :obj:`None`)
- `url` _Optional[str], optional_ - The url to the Cohere service.
- `(default` - :obj:`None`)
- `token_counter` _Optional[BaseTokenCounter], optional_ - Token counter to
  use for the model. If not provided, :obj:`OpenAITokenCounter(
  ModelType.GPT_4O_MINI)` will be used.
- `(default` - :obj:`None`)
- `timeout` _Optional[float], optional_ - The timeout value in seconds for
  API calls. If not provided, will fall back to the MODEL_TIMEOUT
  environment variable or default to 180 seconds.
- `(default` - :obj:`None`)

<a id="camel.models.cohere_model.CohereModel.token_counter"></a>

#### token\_counter

```python
@property
def token_counter() -> BaseTokenCounter
```

Initialize the token counter for the model backend.

**Returns**:

- `BaseTokenCounter` - The token counter following the model's
  tokenization style.

<a id="camel.models.cohere_model.CohereModel.check_model_config"></a>

#### check\_model\_config

```python
def check_model_config()
```

Check whether the model configuration contains any unexpected
arguments to Cohere API.

**Raises**:

- `ValueError` - If the model configuration dictionary contains any
  unexpected arguments to Cohere API.

<a id="camel.models.cohere_model.CohereModel.stream"></a>

#### stream

```python
@property
def stream() -> bool
```

Returns whether the model is in stream mode, which sends partial
results each time. Current it's not supported.

**Returns**:

- `bool` - Whether the model is in stream mode.


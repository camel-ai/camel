<a id="camel.datagen.cot_datagen"></a>

# camel.datagen.cot\_datagen

<a id="camel.datagen.cot_datagen.AgentResponse"></a>

## AgentResponse Objects

```python
class AgentResponse(BaseModel)
```

Model for structured agent responses.

A Pydantic model class that represents structured responses from agents,
including a similarity score that measures the quality of the response.

**Arguments**:

- `score` _float_ - A similarity score between 0 and 1 that compares the
  current answer to the correct answer. Must be within the range
  [0, 1].

<a id="camel.datagen.cot_datagen.VerificationResponse"></a>

## VerificationResponse Objects

```python
class VerificationResponse(BaseModel)
```

Model for structured verification responses.

A Pydantic model class that represents verification results from agents,
indicating whether an answer is correct or not.

**Arguments**:

- `is_correct` _bool_ - Boolean indicating if the answer is correct.

<a id="camel.datagen.cot_datagen.CoTDataGenerator"></a>

## CoTDataGenerator Objects

```python
class CoTDataGenerator()
```

Class for generating and managing data through chat agent interactions.

This module implements a sophisticated Chain of Thought data generation
system that combines several key algorithms to produce high-quality
reasoning paths. Methods implemented:

1. Monte Carlo Tree Search (MCTS)
2. Binary Search Error Detection
3. Dual-Agent Verification System
4. Solution Tree Management

**Arguments**:

- `chat_agent` _Optional[ChatAgent]_ - Optional single agent
  for both tasks (legacy mode). (default::obj:`None`)
- `generator_agent` _Optional[ChatAgent]_ - Optional specialized agent for
  answer generation. (default::obj:`None`)
- `verifier_agent` _Optional[ChatAgent]_ - Optional specialized agent for
  answer verification. (default::obj:`None`)
- `golden_answers` _Dict[str, str]_ - Dictionary containing pre-defined
  correct answers for validation and comparison. Required for answer
  verification.
- `search_limit` _int_ - Maximum number of search iterations allowed.
  (default::obj:`100`)

<a id="camel.datagen.cot_datagen.CoTDataGenerator.__init__"></a>

#### \_\_init\_\_

```python
def __init__(chat_agent: Optional[ChatAgent] = None,
             *,
             generator_agent: Optional[ChatAgent] = None,
             verifier_agent: Optional[ChatAgent] = None,
             golden_answers: Dict[str, str],
             search_limit: int = 100)
```

Initialize the CoTDataGenerator.

This constructor supports both single-agent and dual-agent modes:
1. Single-agent mode (legacy): Pass a single chat_agent that will be
used for both generation and verification.
2. Dual-agent mode: Pass separate generator_agent and verifier_agent
for specialized tasks.

**Arguments**:

- `chat_agent` _Optional[ChatAgent]_ - Optional single agent for both
  tasks (legacy mode). (default::obj:`None`)
- `generator_agent` _Optional[ChatAgent]_ - Optional specialized agent
  for answer generation. (default::obj:`None`)
- `verifier_agent` _Optional[ChatAgent]_ - Optional specialized agent
  for answer verification. (default::obj:`None`)
- `golden_answers` _Dict[str, str]_ - Dictionary containing pre-defined
  correct answers for validation and comparison. Required for
  answer verification.
- `search_limit` _int_ - Maximum number of search iterations allowed.
  (default::obj:`100`)

<a id="camel.datagen.cot_datagen.CoTDataGenerator.get_answer"></a>

#### get\_answer

```python
def get_answer(question: str, context: str = "") -> str
```

Get an answer from the chat agent for a given question.

**Arguments**:

- `question` _str_ - The question to ask.
- `context` _str_ - Additional context for the question.
  (default::obj:`""`)
  

**Returns**:

- `str` - The generated answer.

<a id="camel.datagen.cot_datagen.CoTDataGenerator.verify_answer"></a>

#### verify\_answer

```python
def verify_answer(question: str, answer: str) -> bool
```

Verify if a generated answer is semantically equivalent to
the golden answer for a given question.

**Arguments**:

- `question` _str_ - The question being answered.
- `answer` _str_ - The answer to verify.
  

**Returns**:

- `bool` - True if the answer matches the golden answer based on
  semantic equivalence (meaning the core content and meaning are
  the same, even if the exact wording differs).
  False in the following cases:
  - If the provided question doesn't exist in the golden answers
  - If the answer's meaning differs from the golden answer

<a id="camel.datagen.cot_datagen.CoTDataGenerator.monte_carlo_tree_search"></a>

#### monte\_carlo\_tree\_search

```python
def monte_carlo_tree_search(question: str,
                            partial_solution: str = "") -> float
```

Perform Monte Carlo Tree Search to find the best solution.

Process:
a. Selection: Choose promising partial solutions based on previous
scores
b. Expansion: Generate new solution steps using the generator agent
c. Simulation: Evaluate solution quality using similarity scores
d. Backpropagation: Update solution tree with new findings

**Arguments**:

- `question` _str_ - The question to solve.
- `partial_solution` _str_ - The current partial solution.
  (default::obj:`""`)
  

**Returns**:

- `float` - The similarity score between the current
  solution and golden answer.

<a id="camel.datagen.cot_datagen.CoTDataGenerator.binary_search_error"></a>

#### binary\_search\_error

```python
def binary_search_error(question: str, solution: str) -> int
```

Use binary search to locate the first error in the solution.
This method splits the solution into sentences using both English and
Chinese sentence delimiters and performs binary search to find the
first error.

**Arguments**:

- `question` _str_ - The question being solved.
- `solution` _str_ - The complete solution to analyze.
  

**Returns**:

- `int` - The position of the first error found in the solution.
  Returns -1. If no errors are found (all sentences are correct).

<a id="camel.datagen.cot_datagen.CoTDataGenerator.solve"></a>

#### solve

```python
def solve(question: str) -> str
```

Solve a question using a multi-step approach.

The solution process follows these steps:
1. Try to solve directly - if correct, return the solution
2. If not correct, use Monte Carlo Tree Search to find a good solution
3. If the solution isn't perfect, use binary search to locate errors
4. Generate a new solution based on the correct part

**Arguments**:

- `question` _str_ - The question to solve.
  

**Returns**:

- `str` - The best solution found.

<a id="camel.datagen.cot_datagen.CoTDataGenerator.import_qa_from_json"></a>

#### import\_qa\_from\_json

```python
def import_qa_from_json(data: Union[str, Dict[str, str]]) -> bool
```

Import question and answer data from either a JSON file or a
dictionary.

**Arguments**:

- `data` _Union[str, Dict[str, str]]_ - Either a path to a JSON file
  containing QA pairs or a dictionary of question-answer pairs.
  If a string is provided, it's treated as a file path.
  The expected format is:
- `{"question1"` - "answer1",
- `"question2"` - "answer2",
  ...}
  

**Returns**:

- `bool` - True if import was successful, False otherwise.

<a id="camel.datagen.cot_datagen.CoTDataGenerator.export_solutions"></a>

#### export\_solutions

```python
def export_solutions(filepath: str = 'solutions.json') -> None
```

Export the solution process and results to a JSON file.
Exports the solution tree, golden answers,
and export timestamp to a JSON file.
The exported data includes:
- solutions: The solution tree
with intermediate steps
- golden_answers: The reference answers used for verification
- export_time: ISO format timestamp of the export

**Arguments**:

- `filepath` _str, optional_ - Path where the JSON file will be saved.
  (default::obj:`'solutions.json'`)
  

**Returns**:

- `None` - The method writes to a file and logs the result but does not
  return any value.

